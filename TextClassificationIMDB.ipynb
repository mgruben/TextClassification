{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "This notebook is intended to showcase a text classification proof-of-concept model, developed in TensorFlow.\n",
    "\n",
    "The dataset used for this notebook is the [Stanford Large Movie Review (IMDB) dataset](http://ai.stanford.edu/~amaas/data/sentiment/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Exploration and Analysis\n",
    "First, let's load our dataset into a `pandas` dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews = pd.read_csv(\"data/reviews.txt\", delimiter=\"\\n\", header=None, names=[\"review\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = pd.read_csv(\"data/labels.txt\", delimiter=\"\\n\", header=None, names=[\"label\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's get an idea of what the dataset looks like here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>bromwell high is a cartoon comedy . it ran at ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>story of a man who has unnatural feelings for ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>homelessness  or houselessness as george carli...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>airport    starts as a brand new luxury    pla...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>brilliant over  acting by lesley ann warren . ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review\n",
       "0  bromwell high is a cartoon comedy . it ran at ...\n",
       "1  story of a man who has unnatural feelings for ...\n",
       "2  homelessness  or houselessness as george carli...\n",
       "3  airport    starts as a brand new luxury    pla...\n",
       "4  brilliant over  acting by lesley ann warren . ..."
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reviews.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      label\n",
       "0  positive\n",
       "1  negative\n",
       "2  positive\n",
       "3  negative\n",
       "4  positive"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hmm, it looks like **review** can't fully fit in the above table.  \n",
    "Let's grab the first review and see what it looks like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'bromwell high is a cartoon comedy . it ran at the same time as some other programs about school life  such as  teachers  . my   years in the teaching profession lead me to believe that bromwell high  s satire is much closer to reality than is  teachers  . the scramble to survive financially  the insightful students who can see right through their pathetic teachers  pomp  the pettiness of the whole situation  all remind me of the schools i knew and their students . when i saw the episode in which a student repeatedly tried to burn down the school  i immediately recalled . . . . . . . . . at . . . . . . . . . . high . a classic line inspector i  m here to sack one of your teachers . student welcome to bromwell high . i expect that many adults of my age think that bromwell high is far fetched . what a pity that it isn  t   '"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reviews['review'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "832"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(reviews['review'][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's a good length movie review!  \n",
    "Let's check how many reviews there are in the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25000"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(reviews)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The [Stanford Large Movie Review (IMDB) dataset](http://ai.stanford.edu/~amaas/data/sentiment/) is divided into positive and negative reviews.  \n",
    "Let's check how many positive, and how many negative, reviews there are in this dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7fb93dcc6208>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYgAAAD8CAYAAABthzNFAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMS4xLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvAOZPmwAAFClJREFUeJzt3X+w3XV95/Hna4koYjUI3Ts0oQ2r2bootdU7iOtO59bsQLSdhtmiC4slWGYzTtHVSseG3Z2hq9LRsZRVrNpsyRK7WRHZOmEFxSx6t1tn+aks4YfIHcCSDIo1gL1atbHv/eN8Uo/xE5Kcc3PPRZ6PmTP3831/P9/v93PuH5/X/f4456aqkCRpX/9o0gOQJC1NBoQkqcuAkCR1GRCSpC4DQpLUZUBIkroMCElSlwEhSeoyICRJXcsmPYBRHXfccbVq1aqRtv32t7/N0UcfvbADkqRFMO78dfvtt/91Vf30wfR9ygbEqlWruO2220badnZ2lpmZmYUdkCQtgnHnryRfPdi+XmKSJHUZEJKkLgNCktRlQEiSugwISVKXASFJ6jIgJEldBoQkqcuAkCR1PWU/ST2OHbue4LyN1y36cR96z68u+jElHR6rJjCHAFy5dvG+JsgzCElSlwEhSeoyICRJXQcMiCSbkzya5K6h2vuSfDnJnUk+mWT50LqLkswluS/J6UP1ta02l2TjUP3EJDe3+seTHLmQb1CSNJqDOYO4Eli7T2078JKq+gXgK8BFAElOAs4CXty2+VCSI5IcAfwx8BrgJODs1hfgvcBlVfVC4DHg/LHekSRpQRwwIKrqL4Dd+9Q+W1V72uJNwMrWXgdcVVXfq6oHgTnglPaaq6oHqur7wFXAuiQBXg1c07bfApwx5nuSJC2AhbgH8VvAp1t7BfDw0Lqdrba/+rHA40Nhs7cuSZqwsT4HkeQ/AHuArQsznAMebwOwAWBqaorZ2dmR9jN1FFx48p4Dd1xgo45X0tIziTkEYH5+ftHmkpEDIsl5wK8Ba6qqWnkXcMJQt5Wtxn7q3wSWJ1nWziKG+/+YqtoEbAKYnp6uUf/t3uVbt3HpjsX/jOBD58ws+jElHR6T+LAtDD4ot1j/MnmkS0xJ1gLvAH69qr4ztOpa4Kwkz0xyIrAauAW4FVjdnlg6ksGN7GtbsHweOLNtvx7YNtpbkSQtpIN5zPVjwP8Ffj7JziTnAx8EfgrYnuSOJB8BqKq7gauBe4DPABdU1Q/a2cGbgRuAe4GrW1+A3wPenmSOwT2JKxb0HUqSRnLA6yxVdXanvN9JvKouAS7p1K8Hru/UH2DwlJMkaQnxk9SSpC4DQpLUZUBIkroMCElSlwEhSeoyICRJXQaEJKnLgJAkdRkQkqQuA0KS1GVASJK6DAhJUpcBIUnqMiAkSV0GhCSpy4CQJHUZEJKkLgNCktRlQEiSugwISVKXASFJ6jIgJEldBoQkqcuAkCR1HTAgkmxO8miSu4Zqz0+yPcn97ecxrZ4kH0gyl+TOJC8b2mZ9639/kvVD9Zcn2dG2+UCSLPSblCQduoM5g7gSWLtPbSNwY1WtBm5sywCvAVa31wbgwzAIFOBi4BXAKcDFe0Ol9fm3Q9vteyxJ0gQcMCCq6i+A3fuU1wFbWnsLcMZQ/aM1cBOwPMnxwOnA9qraXVWPAduBtW3dc6vqpqoq4KND+5IkTdCo9yCmquqR1v4aMNXaK4CHh/rtbLUnq+/s1CVJE7Zs3B1UVSWphRjMgSTZwODSFVNTU8zOzo60n6mj4MKT9yzgyA7OqOOVtPRMYg4BmJ+fX7S5ZNSA+HqS46vqkXaZ6NFW3wWcMNRvZavtAmb2qc+2+spO/66q2gRsApienq6ZmZn9dX1Sl2/dxqU7xs7GQ/bQOTOLfkxJh8d5G6+byHGvXHs0o859h2rUS0zXAnufRFoPbBuqn9ueZjoVeKJdiroBOC3JMe3m9GnADW3dt5Kc2p5eOndoX5KkCTrgn9FJPsbgr//jkuxk8DTSe4Crk5wPfBV4fet+PfBaYA74DvBGgKraneRdwK2t3zurau+N799m8KTUUcCn20uSNGEHDIiqOns/q9Z0+hZwwX72sxnY3KnfBrzkQOOQJC0uP0ktSeoyICRJXQaEJKnLgJAkdRkQkqQuA0KS1GVASJK6DAhJUpcBIUnqMiAkSV0GhCSpy4CQJHUZEJKkLgNCktRlQEiSugwISVKXASFJ6jIgJEldBoQkqcuAkCR1GRCSpC4DQpLUZUBIkroMCElSlwEhSeoaKyCS/E6Su5PcleRjSZ6V5MQkNyeZS/LxJEe2vs9sy3Nt/aqh/VzU6vclOX28tyRJWggjB0SSFcC/A6ar6iXAEcBZwHuBy6rqhcBjwPltk/OBx1r9staPJCe17V4MrAU+lOSIUcclSVoY415iWgYclWQZ8GzgEeDVwDVt/RbgjNZe15Zp69ckSatfVVXfq6oHgTnglDHHJUka07JRN6yqXUn+EPgr4G+BzwK3A49X1Z7WbSeworVXAA+3bfckeQI4ttVvGtr18DY/IskGYAPA1NQUs7OzI4196ii48OQ9B+64wEYdr6SlZxJzCMD8/PyizSUjB0SSYxj89X8i8DjwCQaXiA6bqtoEbAKYnp6umZmZkfZz+dZtXLpj5Lc+sofOmVn0Y0o6PM7beN1Ejnvl2qMZde47VONcYvqXwINV9Y2q+jvgz4FXAcvbJSeAlcCu1t4FnADQ1j8P+OZwvbONJGlCxgmIvwJOTfLsdi9hDXAP8HngzNZnPbCtta9ty7T1n6uqavWz2lNOJwKrgVvGGJckaQGMcw/i5iTXAF8E9gBfYnD55zrgqiTvbrUr2iZXAH+WZA7YzeDJJarq7iRXMwiXPcAFVfWDUcclSVoYY12Ir6qLgYv3KT9A5ymkqvou8Lr97OcS4JJxxiJJWlh+klqS1GVASJK6DAhJUpcBIUnqMiAkSV0GhCSpy4CQJHUZEJKkLgNCktRlQEiSugwISVKXASFJ6jIgJEldBoQkqcuAkCR1GRCSpC4DQpLUZUBIkroMCElSlwEhSeoyICRJXQaEJKnLgJAkdRkQkqSusQIiyfIk1yT5cpJ7k7wyyfOTbE9yf/t5TOubJB9IMpfkziQvG9rP+tb//iTrx31TkqTxjXsG8X7gM1X1IuClwL3ARuDGqloN3NiWAV4DrG6vDcCHAZI8H7gYeAVwCnDx3lCRJE3OyAGR5HnALwNXAFTV96vqcWAdsKV12wKc0drrgI/WwE3A8iTHA6cD26tqd1U9BmwH1o46LknSwlg2xrYnAt8A/muSlwK3A28Fpqrqkdbna8BUa68AHh7afmer7a/+Y5JsYHD2wdTUFLOzsyMNfOoouPDkPSNtO45Rxytp6ZnEHAIwPz+/aHPJOAGxDHgZ8JaqujnJ+/nh5SQAqqqS1DgD3Gd/m4BNANPT0zUzMzPSfi7fuo1Ld4zz1kfz0Dkzi35MSYfHeRuvm8hxr1x7NKPOfYdqnHsQO4GdVXVzW76GQWB8vV06ov18tK3fBZwwtP3KVttfXZI0QSMHRFV9DXg4yc+30hrgHuBaYO+TSOuBba19LXBue5rpVOCJdinqBuC0JMe0m9OntZokaYLGvc7yFmBrkiOBB4A3Mgidq5OcD3wVeH3rez3wWmAO+E7rS1XtTvIu4NbW751VtXvMcUmSxjRWQFTVHcB0Z9WaTt8CLtjPfjYDm8cZiyRpYflJaklSlwEhSeoyICRJXQaEJKnLgJAkdRkQkqQuA0KS1GVASJK6DAhJUpcBIUnqMiAkSV0GhCSpy4CQJHUZEJKkLgNCktRlQEiSugwISVKXASFJ6jIgJEldBoQkqcuAkCR1GRCSpC4DQpLUZUBIkrrGDogkRyT5UpJPteUTk9ycZC7Jx5Mc2erPbMtzbf2qoX1c1Or3JTl93DFJksa3EGcQbwXuHVp+L3BZVb0QeAw4v9XPBx5r9ctaP5KcBJwFvBhYC3woyRELMC5J0hjGCogkK4FfBf60LQd4NXBN67IFOKO117Vl2vo1rf864Kqq+l5VPQjMAaeMMy5J0vjGPYP4z8A7gL9vy8cCj1fVnra8E1jR2iuAhwHa+ida/3+od7aRJE3IslE3TPJrwKNVdXuSmYUb0pMecwOwAWBqaorZ2dmR9jN1FFx48p4Dd1xgo45X0tIziTkEYH5+ftHmkpEDAngV8OtJXgs8C3gu8H5geZJl7SxhJbCr9d8FnADsTLIMeB7wzaH6XsPb/Iiq2gRsApienq6ZmZmRBn751m1cumOctz6ah86ZWfRjSjo8ztt43USOe+Xaoxl17jtUI19iqqqLqmplVa1icJP5c1V1DvB54MzWbT2wrbWvbcu09Z+rqmr1s9pTTicCq4FbRh2XJGlhHI4/o38PuCrJu4EvAVe0+hXAnyWZA3YzCBWq6u4kVwP3AHuAC6rqB4dhXJKkQ7AgAVFVs8Bsaz9A5ymkqvou8Lr9bH8JcMlCjEWStDD8JLUkqcuAkCR1GRCSpC4DQpLUZUBIkroMCElSlwEhSeoyICRJXQaEJKnLgJAkdRkQkqQuA0KS1GVASJK6DAhJUpcBIUnqMiAkSV0GhCSpy4CQJHUZEJKkLgNCktRlQEiSugwISVKXASFJ6jIgJEldIwdEkhOSfD7JPUnuTvLWVn9+ku1J7m8/j2n1JPlAkrkkdyZ52dC+1rf+9ydZP/7bkiSNa5wziD3AhVV1EnAqcEGSk4CNwI1VtRq4sS0DvAZY3V4bgA/DIFCAi4FXAKcAF+8NFUnS5IwcEFX1SFV9sbX/BrgXWAGsA7a0bluAM1p7HfDRGrgJWJ7keOB0YHtV7a6qx4DtwNpRxyVJWhgLcg8iySrgl4CbgamqeqSt+how1dorgIeHNtvZavurS5ImaNm4O0jyHOB/AG+rqm8l+Yd1VVVJatxjDB1rA4PLU0xNTTE7OzvSfqaOggtP3rNQwzpoo45X0tIziTkEYH5+ftHmkrECIskzGITD1qr681b+epLjq+qRdgnp0VbfBZwwtPnKVtsFzOxTn+0dr6o2AZsApqena2ZmptftgC7fuo1Ld4ydjYfsoXNmFv2Ykg6P8zZeN5HjXrn2aEad+w7VOE8xBbgCuLeq/mho1bXA3ieR1gPbhurntqeZTgWeaJeibgBOS3JMuzl9WqtJkiZonD+jXwX8JrAjyR2t9u+B9wBXJzkf+Crw+rbueuC1wBzwHeCNAFW1O8m7gFtbv3dW1e4xxiVJWgAjB0RV/SWQ/axe0+lfwAX72ddmYPOoY5EkLTw/SS1J6jIgJEldBoQkqcuAkCR1GRCSpC4DQpLUZUBIkroMCElSlwEhSeoyICRJXQaEJKnLgJAkdRkQkqQuA0KS1GVASJK6DAhJUpcBIUnqMiAkSV0GhCSpy4CQJHUZEJKkLgNCktRlQEiSugwISVLXkgmIJGuT3JdkLsnGSY9Hkp7ulkRAJDkC+GPgNcBJwNlJTprsqCTp6W1JBARwCjBXVQ9U1feBq4B1Ex6TJD2tLZWAWAE8PLS8s9UkSROybNIDOBRJNgAb2uJ8kvtG3NVxwF8vzKgOXt672EeU9JPmV9479vz1cwfbcakExC7ghKHlla32I6pqE7Bp3IMlua2qpsfdjyQttsWcv5bKJaZbgdVJTkxyJHAWcO2ExyRJT2tL4gyiqvYkeTNwA3AEsLmq7p7wsCTpaW1JBARAVV0PXL9Ihxv7MpUkTciizV+pqsU6liTpKWSp3IOQJC0xT/uASLI8yW8PLf9MkmsmOSZJ6knypiTntvZ5SX5maN2fLvQ3UDztLzElWQV8qqpeMuGhSNJBSzIL/G5V3Xa4jrHkzyCSrEpyb5L/kuTuJJ9NclSSFyT5TJLbk/yfJC9q/V+Q5KYkO5K8O8l8qz8nyY1JvtjW7f0qj/cAL0hyR5L3tePd1ba5KcmLh8Yym2Q6ydFJNie5JcmXhvYlSV1tbvlykq1tTrsmybOTrGnzyI42rzyz9X9PknuS3JnkD1vt95P8bpIzgWlga5u7jhqan96U5H1Dxz0vyQdb+w1t3rojyZ+078Hbv6pa0i9gFbAH+MW2fDXwBuBGYHWrvQL4XGt/Cji7td8EzLf2MuC5rX0cMAek7f+ufY53V2v/DvCfWvt44L7W/gPgDa29HPgKcPSkf1e+fPlauq82txTwqra8GfiPDL5m6J+22keBtwHHAvfxw6s8y9vP32dw1gAwC0wP7X+2hcZPM/huu731TwP/AvhnwP8EntHqHwLOfbIxL/kziObBqrqjtW9n8Iv+58AnktwB/AmDCRzglcAnWvu/D+0jwB8kuRP4Xwy+62nqAMe9GjiztV8P7L03cRqwsR17FngW8LOH/K4kPd08XFVfaO3/BqxhML99pdW2AL8MPAF8F7giyb8CvnOwB6iqbwAPJDk1ybHAi4AvtGO9HLi1zV1rgH/yZPtaMp+DOIDvDbV/wGBif7yqfvEQ9nEOg2R9eVX9XZKHGEzs+1VVu5J8M8kvAP+awRkJDMLmN6pq1O+CkvT0tO9N38cZnC38aKfBh4dPYTCJnwm8GXj1IRznKgZ/1H4Z+GRVVZIAW6rqooPdyVPlDGJf3wIeTPI6gAy8tK27CfiN1j5raJvnAY+2cPgVfviFVX8D/NSTHOvjwDuA51XVna12A/CW9gsnyS+N+4YkPS38bJJXtva/AW4DViV5Yav9JvC/kzyHwZxzPYNL3S/98V096dz1SQb/MuFsBmEBg8vyZyb5xwBJnp/kSb+476kaEDA4Izg/yf8D7uaH/z/ibcDb26WkFzI4VQPYCkwn2QGcyyBZqapvAl9IctfwjZ0h1zAImquHau8CngHcmeTutixJB3IfcEGSe4FjgMuANzK4XL4D+HvgIwwm/k+1eewvgbd39nUl8JG9N6mHV1TVY8C9wM9V1S2tdg+Dex6fbfvdzg8vzXf9xD3mmuTZwN+2U6qzGNyw9ikjSRP1VHyk/qlyD+JQvBz4YLv88zjwWxMejyQ9Jf3EnUFIkhbGU/kehCTpMDIgJEldBoQkqcuAkCR1GRCSpC4DQpLU9f8BbBEiPurJ5WIAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fb93e27aef0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "labels['label'].hist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Excellent, the dataset is already balanced.  \n",
    "Having a balanced dataset makes it easier to train a deep learning model, because it's less likely that the model will focus on one of the classes, and ignore the other.\n",
    "\n",
    "Now, let's see what the distribution of review lengths is, in numbers of characters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7fb93e19a9e8>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fb93dffe160>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "reviews['review'].apply(len).hist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, there are some much longer reviews in this dataset than the ~800 character review above, but most of them are under 2,500 characters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing\n",
    "The problem we are trying to solve here is called **sentiment analysis**, that is, whether the given text is positive or negative.\n",
    "\n",
    "More broadly, this is a type of machine learning problem known as classification.\n",
    "\n",
    "There are numerous different ways to think about the above dataset.  \n",
    "We can think of it as a collection of characters, a sequence of characters, a collection of words, or a sequence of words.  \n",
    "It turns out that the sequence-of-words approach has gotten good results on sentiment analysis problems, so let's think about the above dataset in those terms.\n",
    "\n",
    "Let's consider another review in the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'this film lacked something i couldn  t put my finger on at first charisma on the part of the leading actress . this inevitably translated to lack of chemistry when she shared the screen with her leading man . even the romantic scenes came across as being merely the actors at play . it could very well have been the director who miscalculated what he needed from the actors . i just don  t know .  br    br   but could it have been the screenplay  just exactly who was the chef in love with  he seemed more enamored of his culinary skills and restaurant  and ultimately of himself and his youthful exploits  than of anybody or anything else . he never convinced me he was in love with the princess .  br    br   i was disappointed in this movie . but  don  t forget it was nominated for an oscar  so judge for yourself .  '"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reviews['review'][5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization\n",
    "What we would like to do is to turn the text above into a fixed-length sequence of numbers, which is what our TensorFlow model will expect in order to train on.\n",
    "\n",
    "The basic idea behind working with text data is that we want to create a number for each unique \"token\" that we see, then we can just substitute a \"token\"'s number for the token itself, and arrive at a fixed-length sequence of numbers for TensorFlow to work with.\n",
    "\n",
    "As can be seen above, it's not just words that are present, but also punctuation.  \n",
    "\n",
    "Thankfully, the [natural language toolkit](http://www.nltk.org/) is excellent at turning a string of text into a list of tokens.\n",
    "\n",
    "Let's see how it behaves on the above review:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['this',\n",
       " 'film',\n",
       " 'lacked',\n",
       " 'something',\n",
       " 'i',\n",
       " 'couldn',\n",
       " 't',\n",
       " 'put',\n",
       " 'my',\n",
       " 'finger',\n",
       " 'on',\n",
       " 'at',\n",
       " 'first',\n",
       " 'charisma',\n",
       " 'on',\n",
       " 'the',\n",
       " 'part',\n",
       " 'of',\n",
       " 'the',\n",
       " 'leading',\n",
       " 'actress',\n",
       " '.',\n",
       " 'this',\n",
       " 'inevitably',\n",
       " 'translated']"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.tokenize.word_tokenize(reviews['review'][5])[:25]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above is perfectly sufficient tokenization for our purposes.  \n",
    "Perhaps it could be improved upon slightly, but this breaks up our review into enough unique tokens that our model will be able to learn which ones to use and which ones to discount (more on that in the [Architecture](#Model-Architecture) section)\n",
    "\n",
    "Now, let's create a set of all the words in all of the reviews in our dataset.  \n",
    "We will use this set to create a lookup table of tokens to their \"IDs\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "review_word_set = set()\n",
    "for r in reviews['review']:\n",
    "    review_word_set.update(nltk.tokenize.word_tokenize(r))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "74069"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(review_word_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Excellent, in our 25,000-review dataset, only some ~74,000 unique \"tokens\" have been used.  \n",
    "## Token Encoding\n",
    "Now, let's use this set to assign IDs to our tokens, so that we can easily go from a token to its ID, and vice versa.\n",
    "\n",
    "First though, let's reserve ID `0` for reasons that will become clear in the [Padding](#Padding) section below, and start at ID `1`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_to_id = {word: i for i, word in enumerate(review_word_set, 1)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'joins': 1,\n",
       " 'lutzky': 2,\n",
       " 'completley': 3,\n",
       " 'shamefull': 4,\n",
       " 'yoo': 5,\n",
       " 'ghettoisation': 6,\n",
       " 'tailor': 7,\n",
       " 'roderigo': 8,\n",
       " 'jowls': 9,\n",
       " 'constructed': 10,\n",
       " 'roast': 11,\n",
       " 'millenial': 12,\n",
       " 'bleaked': 13,\n",
       " 'encircle': 14,\n",
       " 'astronomers': 15,\n",
       " 'membership': 16,\n",
       " 'soda': 17,\n",
       " 'matinees': 18,\n",
       " 'rallying': 19,\n",
       " 'clapping': 20,\n",
       " 'deliberate': 21,\n",
       " 'endeavouring': 22,\n",
       " 'bigscreen': 23,\n",
       " 'takeaway': 24,\n",
       " 'brutalizing': 25,\n",
       " 'devon': 26,\n",
       " 'unrequited': 27,\n",
       " 'unkillable': 28,\n",
       " 'bleibteu': 29,\n",
       " 'prollific': 30,\n",
       " 'thrifty': 31,\n",
       " 'clairvoyant': 32,\n",
       " 'avenging': 33,\n",
       " 'tereza': 34,\n",
       " 'psm': 35,\n",
       " 'spellcasting': 36,\n",
       " 'lem': 37,\n",
       " 'alsobrook': 38,\n",
       " 'monogamistic': 39,\n",
       " 'vizier': 40,\n",
       " 'kathleen': 41,\n",
       " 'charlize': 42,\n",
       " 'retailers': 43,\n",
       " 'acception': 44,\n",
       " 'interlinking': 45,\n",
       " 'robes': 46,\n",
       " 'terrifies': 47,\n",
       " 'ambushing': 48,\n",
       " 'squeeze': 49,\n",
       " 'abhi': 50,\n",
       " 'youve': 51,\n",
       " 'rodeo': 52,\n",
       " 'genre': 53,\n",
       " 'lambada': 54,\n",
       " 'volume': 55,\n",
       " 'disloyalty': 56,\n",
       " 'tapdancing': 57,\n",
       " 'assedness': 58,\n",
       " 'orthographic': 59,\n",
       " 'dosage': 60,\n",
       " 'wasters': 61,\n",
       " 'postpone': 62,\n",
       " 'artery': 63,\n",
       " 'savagery': 64,\n",
       " 'palpitation': 65,\n",
       " 'hur': 66,\n",
       " 'melora': 67,\n",
       " 'khrzhanosvky': 68,\n",
       " 'harbouring': 69,\n",
       " 'scuffle': 70,\n",
       " 'chiseled': 71,\n",
       " 'surveyed': 72,\n",
       " 'escapeuntil': 73,\n",
       " 'captains': 74,\n",
       " 'sappho': 75,\n",
       " 'ise': 76,\n",
       " 'rim': 77,\n",
       " 'kiesche': 78,\n",
       " 'kayla': 79,\n",
       " 'boundaries': 80,\n",
       " 'clunky': 81,\n",
       " 'mmhm': 82,\n",
       " 'amaze': 83,\n",
       " 'threatening': 84,\n",
       " 'floss': 85,\n",
       " 'comfy': 86,\n",
       " 'shittttttttttttttty': 87,\n",
       " 'muggings': 88,\n",
       " 'medichlorians': 89,\n",
       " 'campion': 90,\n",
       " 'relationships': 91,\n",
       " 'communicators': 92,\n",
       " 'deadliest': 93,\n",
       " 'fetus': 94,\n",
       " 'chews': 95,\n",
       " 'liar': 96,\n",
       " 'disassociative': 97,\n",
       " 'dismantling': 98,\n",
       " 'dooright': 99,\n",
       " 'suplex': 100,\n",
       " 'parallelisms': 101,\n",
       " 'esqueleto': 102,\n",
       " 'uganda': 103,\n",
       " 'pillar': 104,\n",
       " 'spiritsoh': 105,\n",
       " 'loa': 106,\n",
       " 'deathy': 107,\n",
       " 'tra': 108,\n",
       " 'havel': 109,\n",
       " 'ock': 110,\n",
       " 'felton': 111,\n",
       " 'traversed': 112,\n",
       " 'sucker': 113,\n",
       " 'associate': 114,\n",
       " 'cryptically': 115,\n",
       " 'gangrene': 116,\n",
       " 'kassir': 117,\n",
       " 'dango': 118,\n",
       " 'robbbins': 119,\n",
       " 'headset': 120,\n",
       " 'inveterate': 121,\n",
       " 'bassis': 122,\n",
       " 'asshats': 123,\n",
       " 'clippings': 124,\n",
       " 'simmering': 125,\n",
       " 'lakhan': 126,\n",
       " 'reawakened': 127,\n",
       " 'lubitsch': 128,\n",
       " 'themthe': 129,\n",
       " 'kevetch': 130,\n",
       " 'vandalizing': 131,\n",
       " 'prediction': 132,\n",
       " 'dejectedly': 133,\n",
       " 'colonials': 134,\n",
       " 'diverged': 135,\n",
       " 'herrmann': 136,\n",
       " 'inundated': 137,\n",
       " 'beth': 138,\n",
       " 'ching': 139,\n",
       " 'stoners': 140,\n",
       " 'dekho': 141,\n",
       " 'global': 142,\n",
       " 'severally': 143,\n",
       " 'satnitefever': 144,\n",
       " 'diversity': 145,\n",
       " 'antecedently': 146,\n",
       " 'nicktoons': 147,\n",
       " 'unconvincingly': 148,\n",
       " 'seeding': 149,\n",
       " 'megas': 150,\n",
       " 'abbey': 151,\n",
       " 'villa': 152,\n",
       " 'despaaaaaair': 153,\n",
       " 'sargents': 154,\n",
       " 'leicester': 155,\n",
       " 'zom': 156,\n",
       " 'raters': 157,\n",
       " 'favourably': 158,\n",
       " 'rave': 159,\n",
       " 'maharashtra': 160,\n",
       " 'ambles': 161,\n",
       " 'titilation': 162,\n",
       " 'overcoats': 163,\n",
       " 'serf': 164,\n",
       " 'sototh': 165,\n",
       " 'sizable': 166,\n",
       " 'gonorrhea': 167,\n",
       " 'accredited': 168,\n",
       " 'petter': 169,\n",
       " 'lay': 170,\n",
       " 'reue': 171,\n",
       " 'debunk': 172,\n",
       " 'rippings': 173,\n",
       " 'hh': 174,\n",
       " 'replied': 175,\n",
       " 'ceaseless': 176,\n",
       " 'yeeeeaaaaahhhhhhhhh': 177,\n",
       " 'roarke': 178,\n",
       " 'net': 179,\n",
       " 'slappin': 180,\n",
       " 'superflous': 181,\n",
       " 'melvil': 182,\n",
       " 'hirschbiegel': 183,\n",
       " 'lemorande': 184,\n",
       " 'ryaburi': 185,\n",
       " 'unhappier': 186,\n",
       " 'afterwards': 187,\n",
       " 'jolson': 188,\n",
       " 'atmospherically': 189,\n",
       " 'steelcrafts': 190,\n",
       " 'cones': 191,\n",
       " 'withholds': 192,\n",
       " 'headache': 193,\n",
       " 'languorously': 194,\n",
       " 'teammates': 195,\n",
       " 'demanding': 196,\n",
       " 'debated': 197,\n",
       " 'screening': 198,\n",
       " 'tawa': 199,\n",
       " 'ginger': 200,\n",
       " 'advertisers': 201,\n",
       " 'yau': 202,\n",
       " 'deafness': 203,\n",
       " 'belenguer': 204,\n",
       " 'procreation': 205,\n",
       " 'gypsies': 206,\n",
       " 'mies': 207,\n",
       " 'egomaniacal': 208,\n",
       " 'characther': 209,\n",
       " 'serials': 210,\n",
       " 'demoiselle': 211,\n",
       " 'looker': 212,\n",
       " 'hoke': 213,\n",
       " 'cannell': 214,\n",
       " 'tushar': 215,\n",
       " 'zealous': 216,\n",
       " 'amick': 217,\n",
       " 'paramours': 218,\n",
       " 'flower': 219,\n",
       " 'bruhl': 220,\n",
       " 'increadably': 221,\n",
       " 'edgy': 222,\n",
       " 'specialists': 223,\n",
       " 'prophecies': 224,\n",
       " 'meetings': 225,\n",
       " 'manos': 226,\n",
       " 'goose': 227,\n",
       " 'aruna': 228,\n",
       " 'serrat': 229,\n",
       " 'curiosity': 230,\n",
       " 'kathy': 231,\n",
       " 'waitresses': 232,\n",
       " 'holman': 233,\n",
       " 'straightened': 234,\n",
       " 'sexo': 235,\n",
       " 'keifer': 236,\n",
       " 'honest': 237,\n",
       " 'underwear': 238,\n",
       " 'ragtime': 239,\n",
       " 'grue': 240,\n",
       " 'prominance': 241,\n",
       " 'muncey': 242,\n",
       " 'poland': 243,\n",
       " 'aveu': 244,\n",
       " 'delinquent': 245,\n",
       " 'submissive': 246,\n",
       " 'pup': 247,\n",
       " 'glacial': 248,\n",
       " 'sighs': 249,\n",
       " 'undoing': 250,\n",
       " 'muni': 251,\n",
       " 'anesthetic': 252,\n",
       " 'heeds': 253,\n",
       " 'scripter': 254,\n",
       " 'finishable': 255,\n",
       " 'gdel': 256,\n",
       " 'julliard': 257,\n",
       " 'gamera': 258,\n",
       " 'realist': 259,\n",
       " 'kalmus': 260,\n",
       " 'expositories': 261,\n",
       " 'utility': 262,\n",
       " 'buoyed': 263,\n",
       " 'kenobi': 264,\n",
       " 'temporarily': 265,\n",
       " 'gaydar': 266,\n",
       " 'tanovic': 267,\n",
       " 'brockie': 268,\n",
       " 'bridging': 269,\n",
       " 'torpedoed': 270,\n",
       " 'rinatro': 271,\n",
       " 'programing': 272,\n",
       " 'predictive': 273,\n",
       " 'raimi': 274,\n",
       " 'jittery': 275,\n",
       " 'freestyle': 276,\n",
       " 'murkwood': 277,\n",
       " 'bumped': 278,\n",
       " 'sunflower': 279,\n",
       " 'drawbacks': 280,\n",
       " 'unfrozen': 281,\n",
       " 'unlawful': 282,\n",
       " 'fine': 283,\n",
       " 'brecht': 284,\n",
       " 'mainardi': 285,\n",
       " 'kieth': 286,\n",
       " 'minimally': 287,\n",
       " 'riffen': 288,\n",
       " 'knights': 289,\n",
       " 'thais': 290,\n",
       " 'uplifter': 291,\n",
       " 'strangelove': 292,\n",
       " 'dangerously': 293,\n",
       " 'sustains': 294,\n",
       " 'plinplin': 295,\n",
       " 'farrely': 296,\n",
       " 'excavating': 297,\n",
       " 'rumblefish': 298,\n",
       " 'doosre': 299,\n",
       " 'apotheosis': 300,\n",
       " 'servents': 301,\n",
       " 'sweepstakes': 302,\n",
       " 'newbies': 303,\n",
       " 'plods': 304,\n",
       " 'macrauch': 305,\n",
       " 'sweedish': 306,\n",
       " 'covetous': 307,\n",
       " 'wrecking': 308,\n",
       " 'xvii': 309,\n",
       " 'marlon': 310,\n",
       " 'trustees': 311,\n",
       " 'draub': 312,\n",
       " 'whores': 313,\n",
       " 'viewability': 314,\n",
       " 'critique': 315,\n",
       " 'astounded': 316,\n",
       " 'ruffalo': 317,\n",
       " 'lachlan': 318,\n",
       " 'airmen': 319,\n",
       " 'plothijacking': 320,\n",
       " 'alchemy': 321,\n",
       " 'vladimir': 322,\n",
       " 'scatman': 323,\n",
       " 'parsee': 324,\n",
       " 'pompous': 325,\n",
       " 'bobrick': 326,\n",
       " 'parameters': 327,\n",
       " 'tristran': 328,\n",
       " 'twitch': 329,\n",
       " 'rut': 330,\n",
       " 'backyard': 331,\n",
       " 'oshii': 332,\n",
       " 'retells': 333,\n",
       " 'fought': 334,\n",
       " 'wheat': 335,\n",
       " 'haiduck': 336,\n",
       " 'dn': 337,\n",
       " 'gjs': 338,\n",
       " 'expulsion': 339,\n",
       " 'ave': 340,\n",
       " 'princess': 341,\n",
       " 'wardroom': 342,\n",
       " 'incredable': 343,\n",
       " 'resonate': 344,\n",
       " 'drainpipe': 345,\n",
       " 'macquire': 346,\n",
       " 'caf': 347,\n",
       " 'motorbike': 348,\n",
       " 'fluently': 349,\n",
       " 'followers': 350,\n",
       " 'weighting': 351,\n",
       " 'xine': 352,\n",
       " 'jacuzzi': 353,\n",
       " 'rescuers': 354,\n",
       " 'swamped': 355,\n",
       " 'emilio': 356,\n",
       " 'jewellers': 357,\n",
       " 'berkeley': 358,\n",
       " 'rivera': 359,\n",
       " 'hath': 360,\n",
       " 'vanished': 361,\n",
       " 'higher': 362,\n",
       " 'producing': 363,\n",
       " 'coffers': 364,\n",
       " 'processes': 365,\n",
       " 'blackfoot': 366,\n",
       " 'afrikaners': 367,\n",
       " 'preserved': 368,\n",
       " 'joel': 369,\n",
       " 'grammatical': 370,\n",
       " 'pry': 371,\n",
       " 'mortals': 372,\n",
       " 'garver': 373,\n",
       " 'punctuate': 374,\n",
       " 'amnesia': 375,\n",
       " 'micheaux': 376,\n",
       " 'overambitious': 377,\n",
       " 'kamhi': 378,\n",
       " 'fanfan': 379,\n",
       " 'lonnie': 380,\n",
       " 'antonius': 381,\n",
       " 'slanderous': 382,\n",
       " 'vigo': 383,\n",
       " 'parenthesis': 384,\n",
       " 'vampiros': 385,\n",
       " 'smalltown': 386,\n",
       " 'blazer': 387,\n",
       " 'mcdonnell': 388,\n",
       " 'pollutes': 389,\n",
       " 'demolitions': 390,\n",
       " 'molt': 391,\n",
       " 'visualizing': 392,\n",
       " 'springer': 393,\n",
       " 'batouch': 394,\n",
       " 'pinhead': 395,\n",
       " 'spaceships': 396,\n",
       " 'berenger': 397,\n",
       " 'scratcher': 398,\n",
       " 'croats': 399,\n",
       " 'lenge': 400,\n",
       " 'haitian': 401,\n",
       " 'childish': 402,\n",
       " 'index': 403,\n",
       " 'caterina': 404,\n",
       " 'ao': 405,\n",
       " 'mahmoud': 406,\n",
       " 'angel': 407,\n",
       " 'nonbelieveability': 408,\n",
       " 'favelas': 409,\n",
       " 'abbad': 410,\n",
       " 'gobbled': 411,\n",
       " 'resumes': 412,\n",
       " 'doco': 413,\n",
       " 'farnsworth': 414,\n",
       " 'tanner': 415,\n",
       " 'commit': 416,\n",
       " 'dicing': 417,\n",
       " 'materializes': 418,\n",
       " 'shockingly': 419,\n",
       " 'nelsons': 420,\n",
       " 'wallraff': 421,\n",
       " 'collegego': 422,\n",
       " 'debilitating': 423,\n",
       " 'appalachian': 424,\n",
       " 'treasuring': 425,\n",
       " 'graffitiing': 426,\n",
       " 'mountaineer': 427,\n",
       " 'became': 428,\n",
       " 'coupe': 429,\n",
       " 'hammish': 430,\n",
       " 'panicking': 431,\n",
       " 'bras': 432,\n",
       " 'cheerleader': 433,\n",
       " 'limitations': 434,\n",
       " 'hawkins': 435,\n",
       " 'sophocles': 436,\n",
       " 'abo': 437,\n",
       " 'beatlemaniac': 438,\n",
       " 'thud': 439,\n",
       " 'titlesyou': 440,\n",
       " 'shaggy': 441,\n",
       " 'joysticks': 442,\n",
       " 'schedulers': 443,\n",
       " 'ripa': 444,\n",
       " 'willoughby': 445,\n",
       " 'thow': 446,\n",
       " 'jeri': 447,\n",
       " 'fluid': 448,\n",
       " 'pheobe': 449,\n",
       " 'rapa': 450,\n",
       " 'newport': 451,\n",
       " 'permit': 452,\n",
       " 'barbra': 453,\n",
       " 'meant': 454,\n",
       " 'coslow': 455,\n",
       " 'update': 456,\n",
       " 'brull': 457,\n",
       " 'wabbits': 458,\n",
       " 'vergebens': 459,\n",
       " 'blackmoon': 460,\n",
       " 'wisconsin': 461,\n",
       " 'kubrik': 462,\n",
       " 'e': 463,\n",
       " 'boobies': 464,\n",
       " 'mikhali': 465,\n",
       " 'municipal': 466,\n",
       " 'errol': 467,\n",
       " 'soulplane': 468,\n",
       " 'synonym': 469,\n",
       " 'interwiew': 470,\n",
       " 'paolo': 471,\n",
       " 'dullest': 472,\n",
       " 'cinematographers': 473,\n",
       " 'offering': 474,\n",
       " 'djimon': 475,\n",
       " 'guests': 476,\n",
       " 'knucklehead': 477,\n",
       " 'whatevers': 478,\n",
       " 'cic': 479,\n",
       " 'klimt': 480,\n",
       " 'corns': 481,\n",
       " 'sherawali': 482,\n",
       " 'camillia': 483,\n",
       " 'predators': 484,\n",
       " 'dabbling': 485,\n",
       " 'anjolina': 486,\n",
       " 'rotation': 487,\n",
       " 'catcus': 488,\n",
       " 'scarlatti': 489,\n",
       " 'precedence': 490,\n",
       " 'autonomy': 491,\n",
       " 'dismalness': 492,\n",
       " 'monarchs': 493,\n",
       " 'gendered': 494,\n",
       " 'grabber': 495,\n",
       " 'stills': 496,\n",
       " 'shapeless': 497,\n",
       " 'parsons': 498,\n",
       " 'actorsi': 499,\n",
       " 'beautify': 500,\n",
       " 'camaro': 501,\n",
       " 'cajole': 502,\n",
       " 'splashes': 503,\n",
       " 'formulae': 504,\n",
       " 'balsamic': 505,\n",
       " 'gouden': 506,\n",
       " 'skateboard': 507,\n",
       " 'crorepati': 508,\n",
       " 'convictions': 509,\n",
       " 'brussels': 510,\n",
       " 'painted': 511,\n",
       " 'stockholm': 512,\n",
       " 'eu': 513,\n",
       " 'grower': 514,\n",
       " 'vampishness': 515,\n",
       " 'agrument': 516,\n",
       " 'poliwrath': 517,\n",
       " 'farse': 518,\n",
       " 'lize': 519,\n",
       " 'parini': 520,\n",
       " 'duskfall': 521,\n",
       " 'haggerty': 522,\n",
       " 'anniyan': 523,\n",
       " 'bricklayer': 524,\n",
       " 'sunbathing': 525,\n",
       " 'broek': 526,\n",
       " 'dde': 527,\n",
       " 'freckle': 528,\n",
       " 'brethern': 529,\n",
       " 'fever': 530,\n",
       " 'thespic': 531,\n",
       " 'katzelmacher': 532,\n",
       " 'gottowt': 533,\n",
       " 'surveys': 534,\n",
       " 'nymphomaniac': 535,\n",
       " 'nicolae': 536,\n",
       " 'disorients': 537,\n",
       " 'reinvented': 538,\n",
       " 'dulhaniya': 539,\n",
       " 'wraiths': 540,\n",
       " 'mask': 541,\n",
       " 'candies': 542,\n",
       " 'philharmonic': 543,\n",
       " 'yaaa': 544,\n",
       " 'sustain': 545,\n",
       " 'farfella': 546,\n",
       " 'cassella': 547,\n",
       " 'touts': 548,\n",
       " 'pv': 549,\n",
       " 'formosa': 550,\n",
       " 'graphical': 551,\n",
       " 'dubai': 552,\n",
       " 'extortion': 553,\n",
       " 'blackmails': 554,\n",
       " 'injustices': 555,\n",
       " 'limits': 556,\n",
       " 'resentful': 557,\n",
       " 'oppress': 558,\n",
       " 'collaborating': 559,\n",
       " 'jurassic': 560,\n",
       " 'replace': 561,\n",
       " 'eehaaa': 562,\n",
       " 'symbolist': 563,\n",
       " 'herbut': 564,\n",
       " 'tearful': 565,\n",
       " 'conn': 566,\n",
       " 'schlump': 567,\n",
       " 'mulrooney': 568,\n",
       " 'siskel': 569,\n",
       " 'kaafi': 570,\n",
       " 'ammo': 571,\n",
       " 'pressures': 572,\n",
       " 'perm': 573,\n",
       " 'malayalam': 574,\n",
       " 'adaptaion': 575,\n",
       " 'prefabricated': 576,\n",
       " 'largesse': 577,\n",
       " 'lind': 578,\n",
       " 'agency': 579,\n",
       " 'inadequacies': 580,\n",
       " 'expiating': 581,\n",
       " 'partisan': 582,\n",
       " 'linguistically': 583,\n",
       " 'upgraded': 584,\n",
       " 'reflect': 585,\n",
       " 'totalitarianism': 586,\n",
       " 'tabloid': 587,\n",
       " 'vieques': 588,\n",
       " 'greengrass': 589,\n",
       " 'cthulhu': 590,\n",
       " 'grce': 591,\n",
       " 'timpani': 592,\n",
       " 'dickson': 593,\n",
       " 'nate': 594,\n",
       " 'proficient': 595,\n",
       " 'plotted': 596,\n",
       " 'dondaro': 597,\n",
       " 'shades': 598,\n",
       " 'velveeta': 599,\n",
       " 'mammaries': 600,\n",
       " 'burtis': 601,\n",
       " 'hypothetical': 602,\n",
       " 'mmff': 603,\n",
       " 'ascendance': 604,\n",
       " 'morgues': 605,\n",
       " 'kinghtly': 606,\n",
       " 'henreid': 607,\n",
       " 'shriners': 608,\n",
       " 'blunt': 609,\n",
       " 'conceptwise': 610,\n",
       " 'constructing': 611,\n",
       " 'lakers': 612,\n",
       " 'camion': 613,\n",
       " 'drouin': 614,\n",
       " 'veered': 615,\n",
       " 'ol': 616,\n",
       " 'kuttram': 617,\n",
       " 'hemet': 618,\n",
       " 'catharsis': 619,\n",
       " 'helms': 620,\n",
       " 'pervertish': 621,\n",
       " 'flatly': 622,\n",
       " 'crothers': 623,\n",
       " 'pigs': 624,\n",
       " 'ugarte': 625,\n",
       " 'mastercard': 626,\n",
       " 'austrailan': 627,\n",
       " 'iodine': 628,\n",
       " 'stomach': 629,\n",
       " 'hansel': 630,\n",
       " 'functioned': 631,\n",
       " 'desperate': 632,\n",
       " 'ribbon': 633,\n",
       " 'apparition': 634,\n",
       " 'indispensable': 635,\n",
       " 'parnell': 636,\n",
       " 'minimalistically': 637,\n",
       " 'blundered': 638,\n",
       " 'snatching': 639,\n",
       " 'unshaved': 640,\n",
       " 'wizardry': 641,\n",
       " 'chest': 642,\n",
       " 'abides': 643,\n",
       " 'unfavourable': 644,\n",
       " 'chattering': 645,\n",
       " 'congregate': 646,\n",
       " 'streaked': 647,\n",
       " 'minimal': 648,\n",
       " 'primordial': 649,\n",
       " 'ploughs': 650,\n",
       " 'yalu': 651,\n",
       " 'unpublished': 652,\n",
       " 'alienate': 653,\n",
       " 'hdnet': 654,\n",
       " 'disrobed': 655,\n",
       " 'meanies': 656,\n",
       " 'frilly': 657,\n",
       " 'washout': 658,\n",
       " 'hairstylist': 659,\n",
       " 'unironic': 660,\n",
       " 'diverges': 661,\n",
       " 'mispronunciation': 662,\n",
       " 'barefaced': 663,\n",
       " 'coppery': 664,\n",
       " 'biryani': 665,\n",
       " 'alcantara': 666,\n",
       " 'priority': 667,\n",
       " 'pigtailed': 668,\n",
       " 'damns': 669,\n",
       " 'faso': 670,\n",
       " 'admonish': 671,\n",
       " 'owe': 672,\n",
       " 'duchy': 673,\n",
       " 'bonnie': 674,\n",
       " 'intellectuals': 675,\n",
       " 'falcon': 676,\n",
       " 'islamic': 677,\n",
       " 'newpaper': 678,\n",
       " 'amusedly': 679,\n",
       " 'maillard': 680,\n",
       " 'flattening': 681,\n",
       " 'upgrade': 682,\n",
       " 'questionnaire': 683,\n",
       " 'jays': 684,\n",
       " 'tujunga': 685,\n",
       " 'sexuality': 686,\n",
       " 'regressive': 687,\n",
       " 'clowning': 688,\n",
       " 'coleen': 689,\n",
       " 'wreaked': 690,\n",
       " 'shroud': 691,\n",
       " 'swimmer': 692,\n",
       " 'desperadoes': 693,\n",
       " 'phoniness': 694,\n",
       " 'resolves': 695,\n",
       " 'disabilities': 696,\n",
       " 'treetop': 697,\n",
       " 'deepika': 698,\n",
       " 'kahn': 699,\n",
       " 'siam': 700,\n",
       " 'pollute': 701,\n",
       " 'bobbins': 702,\n",
       " 'sebastain': 703,\n",
       " 'bandy': 704,\n",
       " 'recommand': 705,\n",
       " 'lure': 706,\n",
       " 'backstage': 707,\n",
       " 'punctuating': 708,\n",
       " 'facades': 709,\n",
       " 'bungalow': 710,\n",
       " 'xtro': 711,\n",
       " 'slobbery': 712,\n",
       " 'simpsons': 713,\n",
       " 'carrel': 714,\n",
       " 'precipitously': 715,\n",
       " 'possessingand': 716,\n",
       " 'wagered': 717,\n",
       " 'buggies': 718,\n",
       " 'republican': 719,\n",
       " 'goldhunt': 720,\n",
       " 'confuse': 721,\n",
       " 'measuring': 722,\n",
       " 'searingly': 723,\n",
       " 'abgail': 724,\n",
       " 'unpopulated': 725,\n",
       " 'solution': 726,\n",
       " 'empted': 727,\n",
       " 'shana': 728,\n",
       " 'cull': 729,\n",
       " 'clandestine': 730,\n",
       " 'tories': 731,\n",
       " 'sandt': 732,\n",
       " 'odder': 733,\n",
       " 'polnareff': 734,\n",
       " 'douce': 735,\n",
       " 'humbleness': 736,\n",
       " 'driveway': 737,\n",
       " 'nail': 738,\n",
       " 'fanfaberies': 739,\n",
       " 'megawatt': 740,\n",
       " 'evened': 741,\n",
       " 'titanium': 742,\n",
       " 'rw': 743,\n",
       " 'rotne': 744,\n",
       " 'measurement': 745,\n",
       " 'moroder': 746,\n",
       " 'tree': 747,\n",
       " 'university': 748,\n",
       " 'dueling': 749,\n",
       " 'chasers': 750,\n",
       " 'sloughed': 751,\n",
       " 'paleface': 752,\n",
       " 'upstream': 753,\n",
       " 'finnlayson': 754,\n",
       " 'churchyards': 755,\n",
       " 'casings': 756,\n",
       " 'outlandishly': 757,\n",
       " 'cooperated': 758,\n",
       " 'itt': 759,\n",
       " 'brotherconflict': 760,\n",
       " 'lederer': 761,\n",
       " 'hags': 762,\n",
       " 'crocuses': 763,\n",
       " 'pacios': 764,\n",
       " 'crisscross': 765,\n",
       " 'dabbing': 766,\n",
       " 'diego': 767,\n",
       " 'grinds': 768,\n",
       " 'restful': 769,\n",
       " 'travail': 770,\n",
       " 'avoide': 771,\n",
       " 'profs': 772,\n",
       " 'mum': 773,\n",
       " 'winier': 774,\n",
       " 'isolytic': 775,\n",
       " 'keith': 776,\n",
       " 'cheeks': 777,\n",
       " 'careens': 778,\n",
       " 'singled': 779,\n",
       " 'spikey': 780,\n",
       " 'wellworn': 781,\n",
       " 'monters': 782,\n",
       " 'sweep': 783,\n",
       " 'presson': 784,\n",
       " 'ioffer': 785,\n",
       " 'chaplain': 786,\n",
       " 'najwa': 787,\n",
       " 'afraid': 788,\n",
       " 'patriots': 789,\n",
       " 'critcism': 790,\n",
       " 'ginga': 791,\n",
       " 'magical': 792,\n",
       " 'mwahaha': 793,\n",
       " 'abvious': 794,\n",
       " 'ros': 795,\n",
       " 'lambs': 796,\n",
       " 'batchelor': 797,\n",
       " 'tylenol': 798,\n",
       " 'aji': 799,\n",
       " 'befit': 800,\n",
       " 'billiard': 801,\n",
       " 'directionless': 802,\n",
       " 'tyke': 803,\n",
       " 'fascism': 804,\n",
       " 'tugged': 805,\n",
       " 'menelaus': 806,\n",
       " 'sailplane': 807,\n",
       " 'grinchmas': 808,\n",
       " 'singhs': 809,\n",
       " 'sarajevo': 810,\n",
       " 'emerging': 811,\n",
       " 'loris': 812,\n",
       " 'aubert': 813,\n",
       " 'ebbs': 814,\n",
       " 'bankrolling': 815,\n",
       " 'sweetness': 816,\n",
       " 'mover': 817,\n",
       " 'melancholy': 818,\n",
       " 'alohalani': 819,\n",
       " 'ipanema': 820,\n",
       " 'pugilistic': 821,\n",
       " 'surveying': 822,\n",
       " 'hongurai': 823,\n",
       " 'brisk': 824,\n",
       " 'relinquish': 825,\n",
       " 'unescapable': 826,\n",
       " 'peeking': 827,\n",
       " 'pothead': 828,\n",
       " 'regimental': 829,\n",
       " 'discussion': 830,\n",
       " 'buchholz': 831,\n",
       " 'sullavan': 832,\n",
       " 'predeccesor': 833,\n",
       " 'peckingly': 834,\n",
       " 'marginalised': 835,\n",
       " 'hashed': 836,\n",
       " 'hellhole': 837,\n",
       " 'mania': 838,\n",
       " 'doling': 839,\n",
       " 'taco': 840,\n",
       " 'synergy': 841,\n",
       " 'transsexual': 842,\n",
       " 'rfd': 843,\n",
       " 'doodlebops': 844,\n",
       " 'matriculates': 845,\n",
       " 'posest': 846,\n",
       " 'palladino': 847,\n",
       " 'ives': 848,\n",
       " 'aholes': 849,\n",
       " 'relative': 850,\n",
       " 'masturbated': 851,\n",
       " 'davis': 852,\n",
       " 'djjohn': 853,\n",
       " 'champs': 854,\n",
       " 'edina': 855,\n",
       " 'thigpen': 856,\n",
       " 'nac': 857,\n",
       " 'thuglife': 858,\n",
       " 'fanatics': 859,\n",
       " 'minuets': 860,\n",
       " 'dehumanizes': 861,\n",
       " 'aesir': 862,\n",
       " 'destabilise': 863,\n",
       " 'tentatively': 864,\n",
       " 'handsaw': 865,\n",
       " 'nights': 866,\n",
       " 'hamer': 867,\n",
       " 'honda': 868,\n",
       " 'horrorfilm': 869,\n",
       " 'swishing': 870,\n",
       " 'prs': 871,\n",
       " 'offa': 872,\n",
       " 'shielding': 873,\n",
       " 'lenny': 874,\n",
       " 'mcconnohie': 875,\n",
       " 'tamara': 876,\n",
       " 'audiences': 877,\n",
       " 'gubbarre': 878,\n",
       " 'rishi': 879,\n",
       " 'banded': 880,\n",
       " 'quantity': 881,\n",
       " 'spit': 882,\n",
       " 'liggin': 883,\n",
       " 'dau': 884,\n",
       " 'morbid': 885,\n",
       " 'subhuman': 886,\n",
       " 'plumping': 887,\n",
       " 'morano': 888,\n",
       " 'spectrum': 889,\n",
       " 'frowns': 890,\n",
       " 'detractors': 891,\n",
       " 'base': 892,\n",
       " 'forgives': 893,\n",
       " 'epilogue': 894,\n",
       " 'jogging': 895,\n",
       " 'oshawa': 896,\n",
       " 'dormael': 897,\n",
       " 'laboratory': 898,\n",
       " 'mounds': 899,\n",
       " 'doyon': 900,\n",
       " 'surpring': 901,\n",
       " 'consign': 902,\n",
       " 'freeze': 903,\n",
       " 'villagers': 904,\n",
       " 'caked': 905,\n",
       " 'gums': 906,\n",
       " 'gap': 907,\n",
       " 'shinjuku': 908,\n",
       " 'deckard': 909,\n",
       " 'altron': 910,\n",
       " 'jewry': 911,\n",
       " 'wowzers': 912,\n",
       " 'troops': 913,\n",
       " 'curriculum': 914,\n",
       " 'jennie': 915,\n",
       " 'violinist': 916,\n",
       " 'jeopardized': 917,\n",
       " 'wasteful': 918,\n",
       " 'foetus': 919,\n",
       " 'cottrell': 920,\n",
       " 'brutalized': 921,\n",
       " 'stringer': 922,\n",
       " 'crafty': 923,\n",
       " 'dooget': 924,\n",
       " 'exploded': 925,\n",
       " 'annabelle': 926,\n",
       " 'buckheimer': 927,\n",
       " 'labours': 928,\n",
       " 'inlcuded': 929,\n",
       " 'saucepan': 930,\n",
       " 'wilfred': 931,\n",
       " 'perform': 932,\n",
       " 'toying': 933,\n",
       " 'amritlal': 934,\n",
       " 'hoochie': 935,\n",
       " 'fetishistic': 936,\n",
       " 'winnie': 937,\n",
       " 'stead': 938,\n",
       " 'struthers': 939,\n",
       " 'dou': 940,\n",
       " 'himself': 941,\n",
       " 'unions': 942,\n",
       " 'scriptural': 943,\n",
       " 'rowers': 944,\n",
       " 'compositor': 945,\n",
       " 'chun': 946,\n",
       " 'ralf': 947,\n",
       " 'eco': 948,\n",
       " 'skittles': 949,\n",
       " 'goofie': 950,\n",
       " 'unmated': 951,\n",
       " 'landlord': 952,\n",
       " 'jfk': 953,\n",
       " 'electronics': 954,\n",
       " 'preyall': 955,\n",
       " 'hebert': 956,\n",
       " 'commenter': 957,\n",
       " 'karva': 958,\n",
       " 'hoon': 959,\n",
       " 'fourteen': 960,\n",
       " 'joining': 961,\n",
       " 'popularity': 962,\n",
       " 'enthusiastic': 963,\n",
       " 'foibles': 964,\n",
       " 'babaji': 965,\n",
       " 'terrible': 966,\n",
       " 'cranky': 967,\n",
       " 'shrills': 968,\n",
       " 'vittorio': 969,\n",
       " 'miserably': 970,\n",
       " 'fiasco': 971,\n",
       " 'nitta': 972,\n",
       " 'wrestling': 973,\n",
       " 'caracortada': 974,\n",
       " 'incorrigible': 975,\n",
       " 'juttering': 976,\n",
       " 'saranden': 977,\n",
       " 'widening': 978,\n",
       " 'plat': 979,\n",
       " 'pumpkin': 980,\n",
       " 'rocca': 981,\n",
       " 'glassy': 982,\n",
       " 'bullst': 983,\n",
       " 'militants': 984,\n",
       " 'eibon': 985,\n",
       " 'enthusiast': 986,\n",
       " 'cry': 987,\n",
       " 'hearty': 988,\n",
       " 'uped': 989,\n",
       " 'target': 990,\n",
       " 'blatty': 991,\n",
       " 'mir': 992,\n",
       " 'accountable': 993,\n",
       " 'fiber': 994,\n",
       " 'ina': 995,\n",
       " 'oprah': 996,\n",
       " 'thong': 997,\n",
       " 'dirk': 998,\n",
       " 'biographer': 999,\n",
       " 'conquer': 1000,\n",
       " ...}"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_to_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Excellent, we're much closer to being able to having our dataset in a form that TensorFlow can work with.  \n",
    "Just for sanity, let's see what the review above would look like as a list of the IDs above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[64425, 45592, 14606, 17254, 3039, 12082, 33021, 24031, 44721, 12911]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[word_to_id[word] for word in nltk.tokenize.word_tokenize(reviews['review'][5])][:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'm going to trust that the above is mostly correct, but let's at least check that `this` matches the first index in the above list:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "64425"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_to_id[\"this\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perfect.  Now, let's convert all of the tokenized reviews into lists of `ID`s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews_as_ids = []\n",
    "for review in reviews['review']:\n",
    "    reviews_as_ids.append([word_to_id[word] for word in nltk.tokenize.word_tokenize(review)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, as a sanity check, the 6th item (0-indexed!) in the `reviews_as_ids` list should be the same as the list that we see two cells above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[64425, 45592, 14606, 17254, 3039, 12082, 33021, 24031, 44721, 12911]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reviews_as_ids[5][:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Excellent, we have now transformed our dataset of movie reviews into a list of integers.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Label Encoding\n",
    "Now, let's generate an array that we can use with TensorFlow from the labels in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      label\n",
       "0  positive\n",
       "1  negative\n",
       "2  positive\n",
       "3  negative\n",
       "4  positive"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>24995</th>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24996</th>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24997</th>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24998</th>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24999</th>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          label\n",
       "24995  negative\n",
       "24996  positive\n",
       "24997  negative\n",
       "24998  positive\n",
       "24999  negative"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have a list of the reviews which have been integer-tokenized, now let's generate a parallel numpy array of whether the review is positive or negative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "review_labels = np.array(labels['label'] == 'positive', dtype=int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0, 1, 0, 1])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "review_labels[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 0, 1, 0])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "review_labels[-5:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a sanity check, the length of this numpy array should be `25000`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25000"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(review_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Padding\n",
    "The reviews in our dataset contain a different length of tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7fb939818860>"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fb939826908>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pd.Series(reviews_as_ids).apply(len).hist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most of our reviews are under ~500 tokens long, and a few are over 1,200 tokens.\n",
    "\n",
    "The problem is that TensorFlow accepts only inputs of fixed length.  \n",
    "Further, LSTMs often have a practical limitation of only working with sequences that are a few hundred items long.\n",
    "\n",
    "This requires us to pick **a single token length** for our reviews as inputs.  \n",
    "Reviews which are shorter than this will be padded on the **right** with `0`s, and reviews which are longer will have to be truncated before being fed to TensorFlow.\n",
    "\n",
    "This is not as serious a limitation as it sounds, but it does have to be dealt with, or TensorFlow will throw errors during training.\n",
    "\n",
    "Accordingly, let's define a **sequence length** that we will fit every review into."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_len = 200\n",
    "\n",
    "x = np.zeros((len(reviews_as_ids), seq_len), dtype=int)\n",
    "for i, review_as_id in enumerate(reviews_as_ids):\n",
    "    x[i, :len(review_as_id)] = np.array(review_as_id)[:seq_len]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Shown below are two such sequences, one originally shorter than the sequence length, and one longer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([23260, 59862, 48899, 48115, 65297, 23260, 26549, 22618, 54146,\n",
       "        72749, 73035, 13906, 23260, 61768, 73323, 23260, 58446, 15032,\n",
       "        72749, 17430, 59862, 48899, 48115, 65297, 52750, 39369, 40784,\n",
       "        72749, 61680, 22618, 37809, 21420, 71831, 48899, 37466,  2428,\n",
       "         2428, 14136, 29515, 52750, 65417, 15032, 54435, 54357, 61289,\n",
       "        72749, 45592, 54357, 49116, 41960, 23097, 71700, 31926, 37466,\n",
       "         7358, 32132,  9571, 32132, 25035, 54298, 14136, 49755, 18544,\n",
       "        73323, 28098, 49755, 24050, 38966,  9026, 51345, 69655, 37466,\n",
       "         2428,  2428, 72749, 52575, 52387, 32366, 71835, 38993, 42330,\n",
       "        40784, 72749, 15374, 69841, 71835,  8807, 37466, 26180, 53389,\n",
       "        65172, 70836, 58183, 17569, 65172, 42014, 72749, 65055, 37466,\n",
       "        19706,  6709, 13214, 42345, 67180, 29696, 16190,  2299, 35255,\n",
       "        24549,  2838, 25448, 23260, 69283, 18395, 51757,  2428,  2428,\n",
       "        23260, 51554, 51545, 53111, 37466, 19706,  6709,  9559, 14155,\n",
       "         8859, 23260, 36140, 54435,  3039,  5967, 64835, 65297, 22315,\n",
       "         9026, 34523, 16288, 42004, 37466, 64425, 54435, 21420, 46855,\n",
       "        54435, 65172, 55033, 37466,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0]),\n",
       " array([26549, 51582, 72749, 32589, 21420, 23260, 40661, 11328, 46708,\n",
       "        73323, 54674,  9484, 65297, 72749, 57219, 37466, 72749, 46708,\n",
       "        51615, 23260, 24050, 20424,  1925, 18616, 22987, 54298, 72749,\n",
       "         4614, 51420, 65659, 16662, 22618, 65904,   941,  9571, 23260,\n",
       "        52222, 63328, 35513, 35149, 15282, 72749, 40216, 65297, 61519,\n",
       "        51640, 20424, 66993,  9026, 29745, 37466,  1870, 72749, 63328,\n",
       "        32366, 66001, 65172, 43983, 45644, 65172, 73374, 10209, 20349,\n",
       "        37466, 72749, 40599, 59790, 35149, 18934, 27701, 48766, 50269,\n",
       "        73323, 37466,  2428,  2428, 26549, 51582, 72749, 32589, 32366,\n",
       "        23260, 68796, 14253, 65172, 15153,  9026,  7755,  9425, 38020,\n",
       "        11728, 20769, 61794,  9026, 33022, 61487, 21577, 72749,  2299,\n",
       "        22618, 70901, 46114, 65172, 66993, 29745,  9026,  1925, 37466,\n",
       "        45233, 61519,  2299, 54357, 27762, 19088, 48899, 65172, 70836,\n",
       "        72749, 19454, 65297, 26549, 51582, 72749, 32589, 14136, 33022,\n",
       "        72749, 31388,  9026, 57315,  4529, 23415, 19423, 73323,  9149,\n",
       "        23888, 15609, 65297, 72749, 43697, 32136, 65297, 72749, 73736,\n",
       "        37466, 23260, 21732, 21169, 24050, 32366,  1256, 70503, 34841,\n",
       "        23260, 36627, 29202, 22618, 32366, 34137, 40784, 72749,  1769,\n",
       "         9026, 17569, 29292,  3914, 45544, 19706, 48766, 56062, 65172,\n",
       "        20969, 17254,  9026, 40510, 65172, 40147, 26180, 37466,  2428,\n",
       "         2428, 72749, 45452,  9425, 11530, 23260, 27582, 20424,  7692,\n",
       "         9425, 24844,  7358, 26180, 63215, 33021, 41686, 23260, 59853,\n",
       "        19706, 62705]))"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[31], x[1506]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Splitting and Shuffling\n",
    "\n",
    "Now that we have transformed our dataset into a format that TensorFlow can work with, we need to split the dataset into training, testing, and validation sets, so that we can better evaluate the model's performance.\n",
    "\n",
    "Let's use numpy to help us randomize here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Use a seed for reproducibility of results\n",
    "np.random.seed(42)\n",
    "permute = np.random.permutation(range(len(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## Generate shuffled versions of our data and labels\n",
    "x_shuffled = x[permute]\n",
    "y_shuffled = review_labels[permute]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Use an 80% : 10% : 10% split into training : testing : validation sets\n",
    "train_split = int(0.8*len(x))\n",
    "test_split = int(0.9*len(x))\n",
    "\n",
    "train_x = x_shuffled[:train_split]\n",
    "test_x = x_shuffled[train_split:test_split]\n",
    "val_x = x_shuffled[test_split:]\n",
    "\n",
    "train_y = y_shuffled[:train_split]\n",
    "test_y = y_shuffled[train_split:test_split]\n",
    "val_y = y_shuffled[test_split:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a sanity check, let's look at the beginning of the split label sets, to confirm that the data has been shuffled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, 1, 1, 1, 1, 0, 1, 0])"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_y[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 1, 1, 0, 0, 1, 1, 0, 0])"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_y[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0, 1, 0, 0, 1, 1, 1, 0, 0])"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_y[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a final sanity check, note that an 80%, 10%, 10% split should yield numpy arrays of size `20000`, `2500`, and `2500`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20000, 2500, 2500)"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_x), len(test_x), len(val_x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Checkpointing\n",
    "We will use this opportunity to save our training, testing, and validation sets to disk, so that we can quickly load them in the future.\n",
    "\n",
    "Should this notebook need to be run in the future, it can start from the [Loading](#Loading) cells below.\n",
    "## Saving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(\"train_x\", train_x)\n",
    "np.save(\"train_y\", train_y)\n",
    "np.save(\"test_x\", test_x)\n",
    "np.save(\"test_y\", test_y)\n",
    "np.save(\"val_x\", val_x)\n",
    "np.save(\"val_y\", val_y)\n",
    "with open(\"review_word_set.pkl\", 'wb') as f:\n",
    "    pickle.dump(review_word_set, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x = np.load(\"train_x.npy\")\n",
    "train_y = np.load(\"train_y.npy\")\n",
    "test_x = np.load(\"test_x.npy\")\n",
    "test_y = np.load(\"test_y.npy\")\n",
    "val_x = np.load(\"val_x.npy\")\n",
    "val_y = np.load(\"val_y.npy\")\n",
    "with open(\"review_word_set.pkl\", 'rb') as f:\n",
    "    review_word_set = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Architecture\n",
    "In this section, we will be defining a model architecture that we will use to \"learn\" the task of sentiment analysis.\n",
    "\n",
    "As a broad overview, this will be a neural network that takes a sequence as input, and produces a value of either 0 (a negative review) or 1 (a positive review).\n",
    "\n",
    "From the model's perspective, it will be seeing a vector of the above-defined **sequence length**, which is comprised of non-negative integers.\n",
    "\n",
    "After the input, the first layer of the model will be \"embedding\" these non-negative integers into a smaller representational space of only a few hundred floating-point numbers, to allow it to learn which sequences are important, and which are mere noise.\n",
    "\n",
    "Following this embedding layer, the \"embedded\" word vector will then pass into two parallel \"stacks\" of Long Short-Term Memory (LSTM) cells.\n",
    "\n",
    "The advantage of an LSTM cell is that, throughout time, it is able to remember better what came several dozen items earlier in the sequence than is a typical \"feed-forward\" network.  A broad overview of the machinery that allows an LSTM cell to accomplish this memory throughout a time sequence is shown below.\n",
    "![LSTM Cell, from colah's blog](http://colah.github.io/posts/2015-08-Understanding-LSTMs/img/LSTM3-chain.png)\n",
    "\n",
    "Since our dataset is a sequence of words, it makes sense that the model would benefit from knowing about both the sequence going forward, and going backward.  Intuitively, the review \"This movie has everything that you would expect.  And nothing you wouldn't,\" starts off very positive, and only at the end of the sequence would we start to think that the review is negative.  Knowing that from the beginning would make parsing the sequence easier.  \n",
    "Language parsing is a [complicated task](https://research.googleblog.com/2016/05/announcing-syntaxnet-worlds-most.html) that is outside the scope of this notebook, but having this bi-directional parsing typically helps models to pick out what's important for sentiment classification.\n",
    "\n",
    "The bi-directional stacked LSTM architecture that we implement below will be similar to (though not exactly the same as) the following architecture:\n",
    "![bi-directional stacked LSTM, from WildML](http://www.wildml.com/wp-content/uploads/2015/09/Screen-Shot-2015-09-16-at-2.21.51-PM.png)\n",
    "\n",
    "In the above architecture, each successive black dot may be thought of as a token in our reviews.  After embedding this token, we then pass it to a forward group of LSTM cells, and a backward stack of LSTM cells (both in orange), and then perhaps to more of the same \"stacked\" on top, which then generates outputs (red) at the top.\n",
    "\n",
    "We then connect the red \"output\" cells to a single cell, since we want a single answer from the model, namely whether the given input review has a positive or a negative sentiment.\n",
    "\n",
    "Thankfully, TensorFlow provides an API to implement the above, which we will explore below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Placeholders\n",
    "Let's define the necessary placeholders in order for us to feed our pre-processed data from above into a TensorFlow model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/human/miniconda3/envs/ProofOfConcept/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph = tf.Graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "with graph.as_default():\n",
    "    # Define placeholders to accept batches of token-sequences\n",
    "    inputs = tf.placeholder(tf.int32, [None, None], name = \"inputs\")\n",
    "    labels = tf.placeholder(tf.int32, [None, None], name = \"labels\")\n",
    "    \n",
    "    # Define a placeholder to allow us to tweak the dropout hyperparameter\n",
    "    keep_prob = tf.placeholder(tf.float32, name=\"keep_prob\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embedding\n",
    "Many of the unique tokens that exist in our dataset, and thus have been assigned an `ID`, occur rarely.  \n",
    "Things that occur rarely don't give a deep learning model much chance to learn what they mean, and so generally they hurt performance unless we take special steps to combat them.\n",
    "\n",
    "As it turns out, we can probably condense the above ~47,000 unique tokens into a few hundred tokens, and still have the model perform well.  \n",
    "\n",
    "This is done through the use of an [\"embedding\"](https://www.tensorflow.org/versions/master/tutorials/seq2seq#embedding) layer, which lets our model decide which of the tokens it sees are important, and which are just noise.\n",
    "\n",
    "Thankfully, TensorFlow provides a compact way to define and use embeddings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_size = 300\n",
    "\n",
    "with graph.as_default():\n",
    "    # Define an embedding that allows us to map our ~47,000 review tokens to our embedding size\n",
    "    # Our model will \"learn\" a good embedding during training\n",
    "    embedding = tf.Variable(tf.random_uniform((len(review_word_set) + 1, embedding_size), -1, 1), name=\"Embedding\")\n",
    "    embed = tf.nn.embedding_lookup(embedding, inputs, name=\"Embedding_Lookup\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoder\n",
    "Now that we have defined an embedding to squash our input token sequences into a more manageable space, we now define an encoder that will be the heart of this bi-directional stacked LSTM model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_lstm_layers = 2\n",
    "num_lstm_units = 256\n",
    "\n",
    "with graph.as_default():\n",
    "    \n",
    "    def getLSTMlist(num_layers):\n",
    "        \"\"\"\n",
    "        Returns a list containing num_layers LSTM cells,\n",
    "        wrapped in dropout.\n",
    "        \n",
    "        This is to avoid TensorFlow syntax errors with the\n",
    "        [cell] * num_layers construction\n",
    "        \"\"\"\n",
    "        cell_list = []\n",
    "        for _ in range(num_layers):\n",
    "            # Your basic LSTM cell, with L2 regularization\n",
    "            lstm = tf.contrib.rnn.BasicLSTMCell(num_units=num_lstm_units)\n",
    "            \n",
    "            # Add dropout to the cell, using the keep_prob tensor\n",
    "            drop = tf.contrib.rnn.DropoutWrapper(lstm, output_keep_prob=keep_prob)\n",
    "            cell_list.append(drop)\n",
    "            \n",
    "        return cell_list\n",
    "    \n",
    "    forward_LSTMs = getLSTMlist(num_lstm_layers)\n",
    "    backward_LSTMs = getLSTMlist(num_lstm_layers)\n",
    "    \n",
    "    # Stack up multiple bidirectional LSTM layers\n",
    "    # Use dynamic to handle the unpacking of non-sequential input (embed)\n",
    "    outputs, output_state_fw, output_state_bw = \\\n",
    "        tf.contrib.rnn.stack_bidirectional_dynamic_rnn(forward_LSTMs, backward_LSTMs, embed, dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TensorBoard visualizes the above as follows:  \n",
    "![stack_bidirectional_rnn](img/stack.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss Function and Optimizer\n",
    "Finally, we will need to specify a loss function for our model so that it learns properly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 1e-3\n",
    "alpha = 1e-2 # the factor by which to scale our L2 losses\n",
    "use_L2 = False\n",
    "\n",
    "with graph.as_default():\n",
    "    # Squash outputs to a single real number in the range of 0 to 1\n",
    "    predictions = tf.contrib.layers.fully_connected(outputs[:, -1], 1, activation_fn=tf.sigmoid)\n",
    "     \n",
    "    loss = tf.losses.mean_squared_error(labels, predictions)\n",
    "    \n",
    "    # Optimizer for training, using gradient clipping to control exploding gradients\n",
    "    tvars = tf.trainable_variables()\n",
    "    \n",
    "    # Get the L2 losses for weights, kernels, and biases, to discourage memorizing the training set\n",
    "    if use_L2:\n",
    "        for tensor in tvars:\n",
    "            if \"kernel\" in tensor.name or \"weights\" in tensor.name or \"bias\" in tensor.name:\n",
    "                loss += alpha * tf.nn.l2_loss(tensor)\n",
    "    \n",
    "    grads, _ = tf.clip_by_global_norm(tf.gradients(loss, tvars), 5) # clip gradients to 5\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate).minimize(loss, var_list=tvars)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation Operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "with graph.as_default():\n",
    "\n",
    "    ## Round our sigmoid [0,1] output to be either 0 or 1\n",
    "    predictions_integer = tf.cast(tf.round(predictions), tf.int32, name=\"Prediction_Integers\")\n",
    "\n",
    "    ## Define operations for {true, false} {positive, negative} predictions\n",
    "    TP = tf.count_nonzero(predictions_integer * labels, name='True_Positives', dtype=tf.int32)\n",
    "    TN = tf.count_nonzero((predictions_integer - 1) * (labels - 1), name=\"True_Negatives\", dtype=tf.int32)\n",
    "    FP = tf.count_nonzero(predictions_integer * (labels - 1), name=\"False_Positives\", dtype=tf.int32)\n",
    "    FN = tf.count_nonzero((predictions_integer - 1) * labels, name=\"False_Negatives\", dtype=tf.int32)\n",
    "\n",
    "    ## Define operations for accuracy, precision, recall, and F1 score\n",
    "\n",
    "    # accuracy ::= (TP + TN) / (TN + FN + TP + FP)\n",
    "    accuracy = tf.divide(TP + TN, TN + FN + TP + FP, name=\"Accuracy\")\n",
    "\n",
    "    # precision ::= TP / (TP + FP)\n",
    "    precision = tf.divide(TP, TP + FP, name=\"Precision\")\n",
    "\n",
    "    # recall::= TP / (TP + FN)\n",
    "    recall = tf.divide(TP, TP + FN, name=\"Recall\")\n",
    "\n",
    "    # F1 score ::= 2 * precision * recall / (precision + recall)\n",
    "    f1 = tf.divide((2 * precision * recall), (precision + recall), name=\"F1_score\")\n",
    "\n",
    "    # To also record this information in TensorBoard, we'll define scalar summaries here\n",
    "    with tf.name_scope('summaries'):\n",
    "        tf.summary.scalar('Loss', loss)\n",
    "        tf.summary.scalar('Accuracy', accuracy)\n",
    "        tf.summary.scalar('Precision', precision)\n",
    "        tf.summary.scalar('Recall', recall)\n",
    "        tf.summary.scalar('F1_score', f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training the Model\n",
    "We're almost ready to start training our model.\n",
    "\n",
    "Before we begin training, let's define a function that we can use to generate batches of our training data, so that we don't risk running out of memory by loading everything into the model at once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batches(x, y, batch_size=100):\n",
    "    '''\n",
    "    Given a set of features x, and a set of labels y,\n",
    "    return a generator yields shuffled batch_size tuples of x and y.\n",
    "    \n",
    "    Note that this function truncates x and y, if necessary,\n",
    "    so that it only returns full batches.\n",
    "    '''\n",
    "    ## Truncate x and y so that no batch is empty\n",
    "    num_batches = len(x) // batch_size\n",
    "    x, y = x[:num_batches * batch_size], y[:num_batches * batch_size]\n",
    "    \n",
    "    ## Shuffle x and y on the same index permutation\n",
    "    permute = np.random.permutation(range(len(x)))\n",
    "    x = x[permute]\n",
    "    y = y[permute]\n",
    "    \n",
    "    for i in range(0, len(x), batch_size):\n",
    "        yield x[i:i + batch_size], y[i:i + batch_size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To keep our summary directories separate\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  0/10 Iteration:    5 Training loss: 0.255 accuracy: 0.526 precision: 0.530 recall: 0.882 f1: 0.662\n",
      "Epoch:  0/10 Iteration:   10 Training loss: 0.258 accuracy: 0.510 precision: 0.639 recall: 0.228 f1: 0.336\n",
      "Epoch:  0/10 Iteration:   15 Training loss: 0.247 accuracy: 0.542 precision: 0.540 recall: 0.752 f1: 0.629\n",
      "Epoch:  0/10 Iteration:   20 Training loss: 0.250 accuracy: 0.510 precision: 0.539 recall: 0.400 f1: 0.459\n",
      "Epoch:  0/10 Iteration:   25 Training loss: 0.251 accuracy: 0.508 precision: 0.538 recall: 0.273 f1: 0.363\n",
      "                          Validation loss: 0.247 accuracy: 0.527 precision: 0.565 recall: 0.299 f1: 0.390\n",
      "Epoch:  0/10 Iteration:   30 Training loss: 0.249 accuracy: 0.546 precision: 0.551 recall: 0.688 f1: 0.612\n",
      "Epoch:  0/10 Iteration:   35 Training loss: 0.232 accuracy: 0.574 precision: 0.613 recall: 0.421 f1: 0.499\n",
      "Epoch:  0/10 Iteration:   40 Training loss: 0.242 accuracy: 0.562 precision: 0.633 recall: 0.235 f1: 0.342\n",
      "Epoch:  1/10 Iteration:   45 Training loss: 0.254 accuracy: 0.486 precision: 0.498 recall: 0.800 f1: 0.614\n",
      "Epoch:  1/10 Iteration:   50 Training loss: 0.250 accuracy: 0.528 precision: 0.529 recall: 0.646 f1: 0.582\n",
      "                          Validation loss: 0.247 accuracy: 0.542 precision: 0.538 recall: 0.694 f1: 0.606\n",
      "Epoch:  1/10 Iteration:   55 Training loss: 0.249 accuracy: 0.544 precision: 0.557 recall: 0.410 f1: 0.472\n",
      "Epoch:  1/10 Iteration:   60 Training loss: 0.223 accuracy: 0.628 precision: 0.706 recall: 0.448 f1: 0.549\n",
      "Epoch:  1/10 Iteration:   65 Training loss: 0.237 accuracy: 0.582 precision: 0.546 recall: 0.827 f1: 0.658\n",
      "Epoch:  1/10 Iteration:   70 Training loss: 0.238 accuracy: 0.564 precision: 0.557 recall: 0.624 f1: 0.589\n",
      "Epoch:  1/10 Iteration:   75 Training loss: 0.232 accuracy: 0.606 precision: 0.684 recall: 0.450 f1: 0.543\n",
      "                          Validation loss: 0.239 accuracy: 0.583 precision: 0.623 recall: 0.457 f1: 0.527\n",
      "Epoch:  1/10 Iteration:   80 Training loss: 0.227 accuracy: 0.678 precision: 0.679 recall: 0.703 f1: 0.691\n",
      "Epoch:  2/10 Iteration:   85 Training loss: 0.201 accuracy: 0.724 precision: 0.749 recall: 0.696 f1: 0.722\n",
      "Epoch:  2/10 Iteration:   90 Training loss: 0.244 accuracy: 0.576 precision: 0.683 recall: 0.367 f1: 0.478\n",
      "Epoch:  2/10 Iteration:   95 Training loss: 0.241 accuracy: 0.590 precision: 0.636 recall: 0.366 f1: 0.465\n",
      "Epoch:  2/10 Iteration:  100 Training loss: 0.240 accuracy: 0.552 precision: 0.530 recall: 0.855 f1: 0.654\n",
      "                          Validation loss: 0.241 accuracy: 0.561 precision: 0.545 recall: 0.823 f1: 0.655\n",
      "Epoch:  2/10 Iteration:  105 Training loss: 0.220 accuracy: 0.594 precision: 0.569 recall: 0.750 f1: 0.647\n",
      "Epoch:  2/10 Iteration:  110 Training loss: 0.203 accuracy: 0.636 precision: 0.714 recall: 0.557 f1: 0.626\n",
      "Epoch:  2/10 Iteration:  115 Training loss: 0.222 accuracy: 0.634 precision: 0.694 recall: 0.425 f1: 0.527\n",
      "Epoch:  2/10 Iteration:  120 Training loss: 0.247 accuracy: 0.552 precision: 0.550 recall: 0.451 f1: 0.495\n",
      "Epoch:  3/10 Iteration:  125 Training loss: 0.213 accuracy: 0.648 precision: 0.624 recall: 0.758 f1: 0.685\n",
      "                          Validation loss: 0.229 accuracy: 0.605 precision: 0.571 recall: 0.893 f1: 0.697\n",
      "Epoch:  3/10 Iteration:  130 Training loss: 0.211 accuracy: 0.646 precision: 0.661 recall: 0.600 f1: 0.629\n",
      "Epoch:  3/10 Iteration:  135 Training loss: 0.213 accuracy: 0.586 precision: 0.540 recall: 0.747 f1: 0.627\n",
      "Epoch:  3/10 Iteration:  140 Training loss: 0.198 accuracy: 0.666 precision: 0.779 recall: 0.510 f1: 0.616\n",
      "Epoch:  3/10 Iteration:  145 Training loss: 0.212 accuracy: 0.624 precision: 0.594 recall: 0.784 f1: 0.676\n",
      "Epoch:  3/10 Iteration:  150 Training loss: 0.198 accuracy: 0.658 precision: 0.757 recall: 0.496 f1: 0.600\n",
      "                          Validation loss: 0.220 accuracy: 0.619 precision: 0.690 recall: 0.457 f1: 0.549\n",
      "Epoch:  3/10 Iteration:  155 Training loss: 0.196 accuracy: 0.694 precision: 0.847 recall: 0.468 f1: 0.603\n",
      "Epoch:  3/10 Iteration:  160 Training loss: 0.174 accuracy: 0.782 precision: 0.757 recall: 0.822 f1: 0.788\n",
      "Epoch:  4/10 Iteration:  165 Training loss: 0.144 accuracy: 0.808 precision: 0.908 recall: 0.695 f1: 0.788\n",
      "Epoch:  4/10 Iteration:  170 Training loss: 0.179 accuracy: 0.710 precision: 0.908 recall: 0.470 f1: 0.619\n",
      "Epoch:  4/10 Iteration:  175 Training loss: 0.144 accuracy: 0.790 precision: 0.874 recall: 0.647 f1: 0.743\n",
      "                          Validation loss: 0.208 accuracy: 0.697 precision: 0.914 recall: 0.445 f1: 0.599\n",
      "Epoch:  4/10 Iteration:  180 Training loss: 0.156 accuracy: 0.796 precision: 0.726 recall: 0.948 f1: 0.822\n",
      "Epoch:  4/10 Iteration:  185 Training loss: 0.123 accuracy: 0.838 precision: 0.866 recall: 0.817 f1: 0.841\n",
      "Epoch:  4/10 Iteration:  190 Training loss: 0.106 accuracy: 0.862 precision: 0.950 recall: 0.765 f1: 0.848\n",
      "Epoch:  4/10 Iteration:  195 Training loss: 0.132 accuracy: 0.824 precision: 0.897 recall: 0.731 f1: 0.805\n",
      "Epoch:  4/10 Iteration:  200 Training loss: 0.111 accuracy: 0.850 precision: 0.868 recall: 0.830 f1: 0.848\n",
      "                          Validation loss: 0.149 accuracy: 0.808 precision: 0.779 recall: 0.870 f1: 0.822\n",
      "Epoch:  5/10 Iteration:  205 Training loss: 0.109 accuracy: 0.862 precision: 0.828 recall: 0.906 f1: 0.865\n",
      "Epoch:  5/10 Iteration:  210 Training loss: 0.120 accuracy: 0.836 precision: 0.799 recall: 0.890 f1: 0.842\n",
      "Epoch:  5/10 Iteration:  215 Training loss: 0.133 accuracy: 0.818 precision: 0.910 recall: 0.713 f1: 0.799\n",
      "Epoch:  5/10 Iteration:  220 Training loss: 0.120 accuracy: 0.842 precision: 0.877 recall: 0.797 f1: 0.835\n",
      "Epoch:  5/10 Iteration:  225 Training loss: 0.111 accuracy: 0.852 precision: 0.779 recall: 0.943 f1: 0.853\n",
      "                          Validation loss: 0.137 accuracy: 0.818 precision: 0.808 recall: 0.841 f1: 0.824\n",
      "Epoch:  5/10 Iteration:  230 Training loss: 0.081 accuracy: 0.890 precision: 0.879 recall: 0.916 f1: 0.897\n",
      "Epoch:  5/10 Iteration:  235 Training loss: 0.095 accuracy: 0.880 precision: 0.866 recall: 0.894 f1: 0.880\n",
      "Epoch:  5/10 Iteration:  240 Training loss: 0.104 accuracy: 0.864 precision: 0.888 recall: 0.839 f1: 0.863\n",
      "Epoch:  6/10 Iteration:  245 Training loss: 0.087 accuracy: 0.882 precision: 0.848 recall: 0.922 f1: 0.884\n",
      "Epoch:  6/10 Iteration:  250 Training loss: 0.077 accuracy: 0.898 precision: 0.901 recall: 0.898 f1: 0.899\n",
      "                          Validation loss: 0.128 accuracy: 0.829 precision: 0.813 recall: 0.862 f1: 0.837\n",
      "Epoch:  6/10 Iteration:  255 Training loss: 0.083 accuracy: 0.906 precision: 0.945 recall: 0.868 f1: 0.905\n",
      "Epoch:  6/10 Iteration:  260 Training loss: 0.075 accuracy: 0.904 precision: 0.868 recall: 0.941 f1: 0.903\n",
      "Epoch:  6/10 Iteration:  265 Training loss: 0.093 accuracy: 0.874 precision: 0.963 recall: 0.766 f1: 0.853\n",
      "Epoch:  6/10 Iteration:  270 Training loss: 0.074 accuracy: 0.900 precision: 0.904 recall: 0.904 f1: 0.904\n",
      "Epoch:  6/10 Iteration:  275 Training loss: 0.081 accuracy: 0.898 precision: 0.925 recall: 0.870 f1: 0.897\n",
      "                          Validation loss: 0.127 accuracy: 0.828 precision: 0.815 recall: 0.856 f1: 0.834\n",
      "Epoch:  6/10 Iteration:  280 Training loss: 0.057 accuracy: 0.930 precision: 0.910 recall: 0.957 f1: 0.933\n",
      "Epoch:  7/10 Iteration:  285 Training loss: 0.071 accuracy: 0.906 precision: 0.871 recall: 0.952 f1: 0.909\n",
      "Epoch:  7/10 Iteration:  290 Training loss: 0.074 accuracy: 0.916 precision: 0.965 recall: 0.865 f1: 0.912\n",
      "Epoch:  7/10 Iteration:  295 Training loss: 0.072 accuracy: 0.912 precision: 0.891 recall: 0.940 f1: 0.915\n",
      "Epoch:  7/10 Iteration:  300 Training loss: 0.060 accuracy: 0.924 precision: 0.919 recall: 0.926 f1: 0.922\n",
      "                          Validation loss: 0.131 accuracy: 0.828 precision: 0.800 recall: 0.879 f1: 0.838\n",
      "Epoch:  7/10 Iteration:  305 Training loss: 0.067 accuracy: 0.918 precision: 0.932 recall: 0.907 f1: 0.919\n",
      "Epoch:  7/10 Iteration:  310 Training loss: 0.053 accuracy: 0.930 precision: 0.955 recall: 0.908 f1: 0.931\n",
      "Epoch:  7/10 Iteration:  315 Training loss: 0.055 accuracy: 0.934 precision: 0.923 recall: 0.942 f1: 0.933\n",
      "Epoch:  7/10 Iteration:  320 Training loss: 0.049 accuracy: 0.936 precision: 0.950 recall: 0.919 f1: 0.934\n",
      "Epoch:  8/10 Iteration:  325 Training loss: 0.040 accuracy: 0.952 precision: 0.952 recall: 0.944 f1: 0.948\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                          Validation loss: 0.132 accuracy: 0.833 precision: 0.843 recall: 0.826 f1: 0.834\n",
      "Epoch:  8/10 Iteration:  330 Training loss: 0.053 accuracy: 0.936 precision: 0.937 recall: 0.929 f1: 0.933\n",
      "Epoch:  8/10 Iteration:  335 Training loss: 0.053 accuracy: 0.932 precision: 0.925 recall: 0.940 f1: 0.933\n",
      "Epoch:  8/10 Iteration:  340 Training loss: 0.043 accuracy: 0.950 precision: 0.968 recall: 0.922 f1: 0.945\n",
      "Epoch:  8/10 Iteration:  345 Training loss: 0.042 accuracy: 0.950 precision: 0.942 recall: 0.961 f1: 0.951\n",
      "Epoch:  8/10 Iteration:  350 Training loss: 0.049 accuracy: 0.942 precision: 0.968 recall: 0.920 f1: 0.943\n",
      "                          Validation loss: 0.129 accuracy: 0.838 precision: 0.831 recall: 0.854 f1: 0.842\n",
      "Epoch:  8/10 Iteration:  355 Training loss: 0.051 accuracy: 0.942 precision: 0.970 recall: 0.912 f1: 0.940\n",
      "Epoch:  8/10 Iteration:  360 Training loss: 0.033 accuracy: 0.962 precision: 0.948 recall: 0.976 f1: 0.962\n",
      "Epoch:  9/10 Iteration:  365 Training loss: 0.040 accuracy: 0.954 precision: 0.950 recall: 0.954 f1: 0.952\n",
      "Epoch:  9/10 Iteration:  370 Training loss: 0.023 accuracy: 0.976 precision: 0.984 recall: 0.968 f1: 0.976\n",
      "Epoch:  9/10 Iteration:  375 Training loss: 0.045 accuracy: 0.952 precision: 0.951 recall: 0.951 f1: 0.951\n",
      "                          Validation loss: 0.138 accuracy: 0.824 precision: 0.855 recall: 0.789 f1: 0.820\n",
      "Epoch:  9/10 Iteration:  380 Training loss: 0.035 accuracy: 0.952 precision: 0.951 recall: 0.951 f1: 0.951\n",
      "Epoch:  9/10 Iteration:  385 Training loss: 0.027 accuracy: 0.966 precision: 0.959 recall: 0.977 f1: 0.968\n",
      "Epoch:  9/10 Iteration:  390 Training loss: 0.030 accuracy: 0.968 precision: 0.980 recall: 0.957 f1: 0.969\n",
      "Epoch:  9/10 Iteration:  395 Training loss: 0.038 accuracy: 0.960 precision: 0.940 recall: 0.979 f1: 0.959\n",
      "Epoch:  9/10 Iteration:  400 Training loss: 0.040 accuracy: 0.950 precision: 0.971 recall: 0.929 f1: 0.950\n",
      "                          Validation loss: 0.136 accuracy: 0.833 precision: 0.864 recall: 0.797 f1: 0.829\n"
     ]
    }
   ],
   "source": [
    "epochs = 10\n",
    "batch_size = 500\n",
    "train_keep_prob = 0.9\n",
    "\n",
    "# Define a time-stamped directory in which to keep the TensorBoard data for this run\n",
    "summaries_dir = \"./summaries/\" + datetime.now().strftime(\"%Y%m%d%H%M%S\")\n",
    "\n",
    "with graph.as_default():\n",
    "    saver = tf.train.Saver()\n",
    "\n",
    "with tf.Session(graph=graph) as sess:\n",
    "    merged = tf.summary.merge_all()\n",
    "    train_writer = tf.summary.FileWriter(summaries_dir + '/train',\n",
    "                                          sess.graph)\n",
    "    test_writer = tf.summary.FileWriter(summaries_dir + '/test')\n",
    "    val_writer = tf.summary.FileWriter(summaries_dir + '/val')\n",
    "    \n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    iteration = 1\n",
    "    for e in range(epochs):\n",
    "        for x, y in get_batches(train_x, train_y, batch_size):\n",
    "            feed = {inputs: x,\n",
    "                    labels: y[:, None],\n",
    "                    keep_prob: train_keep_prob}\n",
    "            summary, tr_loss, tr_acc, tr_prec, tr_rec, tr_f1, _ = \\\n",
    "                sess.run([merged, loss, accuracy, precision, recall, f1, optimizer], feed_dict=feed)\n",
    "            train_writer.add_summary(summary, iteration)\n",
    "            \n",
    "            # Print training information\n",
    "            if iteration%5==0:\n",
    "                print(\"Epoch:{: 3d}/{}\".format(e, epochs),\n",
    "                      \"Iteration: {: 4d} Training\".format(iteration),\n",
    "                      \"loss: {:.3f}\".format(tr_loss),\n",
    "                     \"accuracy: {:.3f}\".format(tr_acc),\n",
    "                     \"precision: {:.3f}\".format(tr_prec),\n",
    "                     \"recall: {:.3f}\".format(tr_rec),\n",
    "                     \"f1: {:.3f}\".format(tr_f1))\n",
    "            \n",
    "            # Evaluate the validation set\n",
    "            if iteration%25==0:\n",
    "                running_stats = []\n",
    "                for x, y in get_batches(val_x, val_y, batch_size):\n",
    "                    feed = {inputs: x,\n",
    "                            labels: y[:, None],\n",
    "                            keep_prob: 1}\n",
    "                    summary, v_loss, v_acc, v_prec, v_rec, v_f1 = \\\n",
    "                        sess.run([merged, loss, accuracy, precision, recall, f1], feed_dict=feed)\n",
    "                    val_writer.add_summary(summary, iteration)\n",
    "                    running_stats.append([v_loss, v_acc, v_prec, v_rec, v_f1])\n",
    "                running_stats = np.array(running_stats)\n",
    "                val_averages = running_stats.mean(axis=0)\n",
    "                print(\" \"*25,\n",
    "                      \"Validation loss: {:.3f}\".format(val_averages[0]),\n",
    "                     \"accuracy: {:.3f}\".format(val_averages[1]),\n",
    "                     \"precision: {:.3f}\".format(val_averages[2]),\n",
    "                     \"recall: {:.3f}\".format(val_averages[3]),\n",
    "                     \"f1: {:.3f}\".format(val_averages[4]))\n",
    "            iteration +=1\n",
    "    saver.save(sess, \"checkpoints/sentiment.ckpt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluating Performance\n",
    "Below are some screen captures from TensorBoard, to show how the model's Accuracy, Precision, Recall, F1 Score, and Loss progressed on the training and validation sets during training:  \n",
    "![Summaries](img/Summaries.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above, note that the smoothed purple line represents the training set performance, and the jagged red line represents the validation set performance.\n",
    "\n",
    "Now, let's see how our model performed on the test set that we held out earlier.\n",
    "\n",
    "In order to do this, we will plot a \"confusion matrix,\" which concisely displays how many false positives, false negatives, true positives, and true negatives the model yielded.\n",
    "\n",
    "First, let's define our helper code for printing a confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \n",
    "    http://scikit-learn.org/stable/auto_examples/model_selection/plot_confusion_matrix.html#sphx-glr-auto-examples-model-selection-plot-confusion-matrix-py\n",
    "    \"\"\"\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "    print(cm)\n",
    "\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    fmt = '.3f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.8  0.2]\n",
      " [ 0.4  0.6]]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f36c7d78978>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "cnf_matrix = np.array([[8,2],[8,12]])\n",
    "# [TP, FN], [FP, TN]\n",
    "class_names = [\"Positive\", \"Negative\"]\n",
    "\n",
    "np.set_printoptions(precision=3)\n",
    "# Plot normalized confusion matrix\n",
    "\n",
    "plt.Figure()\n",
    "plot_confusion_matrix(cnf_matrix, classes=class_names, normalize=True,\n",
    "                      title='Normalized confusion matrix, Dummy Values')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's reload our model from disk, and run it on the test set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from checkpoints/sentiment.ckpt\n",
      "Test loss: 0.119 accuracy: 0.851 precision: 0.884 recall: 0.810 f1: 0.845\n"
     ]
    }
   ],
   "source": [
    "running_stats = []\n",
    "with tf.Session(graph=graph) as sess:\n",
    "    saver.restore(sess, tf.train.latest_checkpoint('checkpoints'))\n",
    "    conf_mat = np.zeros(4, dtype=int)\n",
    "    running_stats = []\n",
    "    for i, (x, y) in enumerate(get_batches(test_x, test_y, batch_size), 1):\n",
    "        feed = {inputs: x,\n",
    "                labels: y[:, None],\n",
    "                keep_prob: 1}\n",
    "        v_loss, v_acc, v_prec, v_rec, v_f1, true_pos, false_neg, false_pos, true_neg = \\\n",
    "            sess.run([loss, accuracy, precision, recall, f1, TP, FN, FP, TN], feed_dict=feed)\n",
    "        running_stats.append([v_loss, v_acc, v_prec, v_rec, v_f1])\n",
    "        conf_mat += (true_pos, false_neg, false_pos, true_neg)\n",
    "        \n",
    "    running_stats = np.array(running_stats)\n",
    "    test_averages = running_stats.mean(axis=0)\n",
    "    print(\"Test loss: {:.3f}\".format(test_averages[0]),\n",
    "                 \"accuracy: {:.3f}\".format(test_averages[1]),\n",
    "                 \"precision: {:.3f}\".format(test_averages[2]),\n",
    "                 \"recall: {:.3f}\".format(test_averages[3]),\n",
    "                 \"f1: {:.3f}\".format(test_averages[4]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.808  0.192]\n",
      " [ 0.107  0.893]]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f36c4337940>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Reshape into 2 x 2 matrix\n",
    "conf_mat = conf_mat.reshape((2,2))\n",
    "\n",
    "class_names = [\"Positive\", \"Negative\"]\n",
    "\n",
    "# Plot normalized confusion matrix\n",
    "\n",
    "plt.Figure()\n",
    "plot_confusion_matrix(conf_mat, classes=class_names, normalize=True,\n",
    "                      title='Normalized confusion matrix, Test Set')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "As can be seen above, when the model is given a negative review, it classifies that review correctly almost 90% of the time!\n",
    "\n",
    "When the model is given a positive review, its accurate only about 80% of the time, but still that is an incredible first pass.\n",
    "\n",
    "As can also be seen above, we've taken care to gather information about true positives, true negatives, false positives, and false negatives.\n",
    "\n",
    "This allows us to relate the following metrics:\n",
    "\n",
    "![Precision](https://wikimedia.org/api/rest_v1/media/math/render/svg/26106935459abe7c266f7b1ebfa2a824b334c807)\n",
    "\n",
    "![Recall](https://wikimedia.org/api/rest_v1/media/math/render/svg/4c233366865312bc99c832d1475e152c5074891b)\n",
    "\n",
    "![Accuracy](https://wikimedia.org/api/rest_v1/media/math/render/svg/e2e427ec6dcf2d7882c3bbdc659a8204cba59dcc)\n",
    "\n",
    "![F1 Score](https://wikimedia.org/api/rest_v1/media/math/render/svg/dd577aee2dd35c5b0e349327528a5ac606c7bbbf)\n",
    "\n",
    "As can be seen from running the test set above, precision was **0.884**.  \n",
    "Also, recall was **0.810**, accuracy was **0.851**, and the F1 score was **0.845**."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
